---
phase: 03-core-rag-with-hybrid-retrieval
plan: 0
total_plans: 6
status: ready_to_execute
last_updated: 2026-01-28 21:34:07
---

<current_state>
Phase 3 planning is **complete and verified**. All 6 plans created, reviewed, and passed verification checks. Ready to execute the full RAG pipeline implementation.

- Phase 3: Core RAG with Hybrid Retrieval
- Status: Planned (not yet executed)
- Plans: 6 plans in 3 waves
- Next step: Execute Phase 3
</current_state>

<completed_work>

**Phase 2:**
- ✓ Complete document processing pipeline (upload → parse → chunk → index)
- ✓ All 5 plans executed and verified (6/6 must-haves passed)
- ✓ Gap closure completed (docling format mismatch resolved)

**Phase 3 Planning:**
- ✓ Research completed (found Mistral reranker doesn't exist - using Qwen3-Reranker-0.6B)
- ✓ 6 plans created by gsd-planner agent
- ✓ Plans verified by gsd-plan-checker agent
- ✓ Revision cycle completed (7 issues found and fixed)
  - Fixed: RETRIEVAL-08 requirement mismatch (Mistral → Qwen3)
  - Fixed: RRF verification added to indexer
  - Fixed: Explicit indexer.hybrid_search() wiring in retriever
  - Fixed: INDEX-05 coverage (re-ingestion without duplication)
  - Fixed: Split 03-02 into 03-02A + 03-02B to reduce complexity
  - Fixed: Observable truths in must_haves
- ✓ REQUIREMENTS.md and ROADMAP.md updated to reflect Qwen3-Reranker

</completed_work>

<remaining_work>

**Phase 3 Execution (6 plans):**

**Wave 1:**
- [ ] 03-01-PLAN.md - Configuration and chat data models
  - Extend config.py with RAG settings (OpenAI, DeepInfra, LLM params)
  - Create chat.py models (ChatRequest, ChatResponse, Citation, DiagnosticInfo)
  - Update requirements.txt (litellm, openai, redis)

**Wave 2 (parallel):**
- [ ] 03-02A-PLAN.md - Indexer hybrid search capability
  - Enable hybrid=True in txtai config
  - Add hybrid_search() method with RRF fusion
  - Add reindex_document() for INDEX-05 (clean re-ingestion)
  - OpenAI embeddings with fallback to sentence-transformers

- [ ] 03-02B-PLAN.md - Hybrid retriever service
  - Create HybridRetriever class
  - Implement retrieve() that calls indexer.hybrid_search()
  - Add aerospace acronym expansion (UAV, GPS, IMU, etc.)
  - Diagnostic logging (scores, latency)

- [ ] 03-03-PLAN.md - Reranker service
  - Create Reranker class using DeepInfra Qwen3-Reranker-0.6B
  - Implement rerank() via litellm.rerank()
  - Graceful error handling (fallback to original order)

- [ ] 03-04-PLAN.md - Answer generator
  - Create Generator class using OpenAI GPT-5-mini
  - Build grounded prompts with citation numbers [1], [2]
  - Handle empty context gracefully
  - Build Citation objects from chunks

**Wave 3:**
- [ ] 03-05-PLAN.md - Chat endpoint orchestration
  - Create POST /api/chat endpoint
  - Wire full pipeline: retrieve → rerank → generate
  - Diagnostic logging (request_id, latencies)
  - End-to-end verification checkpoint (requires human approval)

</remaining_work>

<decisions_made>

**Architecture:**
- **Reranker model:** Qwen3-Reranker-0.6B (NOT Mistral - research confirmed Mistral has no reranking model)
- **Plan structure:** Split 03-02 into 03-02A (indexer) + 03-02B (retriever) to manage complexity
- **RRF fusion:** txtai's hybrid search with scoring.normalize=True provides RRF-equivalent fusion
- **Three-stage pipeline:** Retrieve (25 chunks) → Rerank (top 12) → Generate (top 10)
- **Hybrid weights:** 50/50 semantic/BM25 by default (configurable)

**Implementation:**
- OpenAI text-embedding-3-small for embeddings (with sentence-transformers fallback)
- DeepInfra Qwen3-Reranker-0.6B via litellm for reranking
- OpenAI GPT-5-mini for answer generation
- Aerospace acronym expansion in retriever (UAV, GPS, IMU, etc.)
- Structured citations with doc_id, filename, page_range, snippet

</decisions_made>

<blockers>

**None currently, but prerequisites required before execution:**

**⚠️ API Keys Required:**
Before running `/gsd:execute-phase 3`, user must configure these API keys in `.env`:

1. **OPENAI_API_KEY** - For embeddings (text-embedding-3-small) and LLM (GPT-5-mini)
   - Get from: https://platform.openai.com/api-keys
   - Used in: Plans 03-02A, 03-04

2. **DEEPINFRA_API_KEY** - For reranking (Qwen3-Reranker-0.6B)
   - Get from: https://deepinfra.com/dash/api_keys
   - Used in: Plan 03-03

**Without these API keys, plans 03-02A, 03-03, and 03-04 will fail.**

</blockers>

<context>

**Mental Model:**

Phase 3 builds the core RAG query pipeline on top of the document ingestion pipeline from Phase 2. Users upload documents (Phase 2), then ask questions about them (Phase 3).

**Pipeline Flow:**
```
User Question
    ↓
1. Hybrid Retrieval (03-02A + 03-02B)
   - Search txtai index with semantic + BM25
   - Retrieve top 25 chunks
    ↓
2. Reranking (03-03)
   - Score with Qwen3-Reranker cross-encoder
   - Keep top 12 chunks
    ↓
3. Generation (03-04)
   - Build prompt with top 10 chunks
   - Call GPT-5-mini with grounding instruction
   - Return answer + citations
    ↓
Chat Response with [1], [2] citation numbers
```

**Wave Structure:**
- Wave 1: Foundation (config + models)
- Wave 2: Service layer (4 services in parallel)
- Wave 3: Integration (chat endpoint orchestrates pipeline)

**Key Insight from Planning:**
The "Mistral reranker" mentioned in original requirements doesn't exist. Research found that Mistral has NO dedicated reranking model. DeepInfra uses Qwen3-Reranker models instead. Requirements and plans updated accordingly.

**Why 6 Plans?**
Original planning had 5 plans but plan-checker found 03-02 was too complex (modifying critical indexer.py + creating new retriever). Split into:
- 03-02A: Indexer modifications only (minimize risk to Phase 2 code)
- 03-02B: New retriever service (depends on 03-02A)

</context>

<next_action>

**When resuming, start with:**

1. **Verify API keys are configured:**
   ```bash
   grep -E 'OPENAI_API_KEY|DEEPINFRA_API_KEY' .env
   ```
   If missing, user must add them before execution.

2. **Execute Phase 3:**
   ```
   /gsd:execute-phase 3
   ```

3. **Expected execution flow:**
   - Wave 1: 03-01 (config + models) - ~10-15 min
   - Wave 2: 03-02A, 03-02B, 03-03, 03-04 (parallel) - ~30-40 min total
   - Wave 3: 03-05 (chat endpoint + checkpoint) - ~15-20 min + human verification

4. **After Phase 3 execution:**
   - Test chat endpoint with uploaded documents
   - Verify citations appear in responses
   - Check performance (<10 seconds per query)
   - Proceed to Phase 4 (Knowledge Graph Integration)

**Command to resume:**
```
/gsd:resume-work
```
or directly:
```
/gsd:execute-phase 3
```

</next_action>

---

**Files Modified During Planning Session:**
- `.planning/REQUIREMENTS.md` - Updated RETRIEVAL-04, RETRIEVAL-08 to specify Qwen3-Reranker
- `.planning/ROADMAP.md` - Updated Phase 3 success criteria and plan list (6 plans)
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-RESEARCH.md` - Created by gsd-phase-researcher
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-01-PLAN.md` - Created by gsd-planner
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-02A-PLAN.md` - Created by gsd-planner (revision)
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-02B-PLAN.md` - Created by gsd-planner (revision)
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-03-PLAN.md` - Created by gsd-planner (revision)
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-04-PLAN.md` - Created by gsd-planner
- `.planning/phases/03-core-rag-with-hybrid-retrieval/03-05-PLAN.md` - Created by gsd-planner (revision)

**Not yet committed to git** - handoff file will be committed below.
