---
phase: 03-core-rag-with-hybrid-retrieval
plan: 04
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - backend/app/services/generator.py
autonomous: true
user_setup:
  - service: openai
    why: "LLM API for answer generation"
    env_vars:
      - name: OPENAI_API_KEY
        source: "OpenAI Dashboard -> API Keys (https://platform.openai.com/api-keys)"

must_haves:
  truths:
    - "Generator calls OpenAI GPT-5-mini API for answer generation"
    - "Generator builds prompt with grounding instruction and numbered chunks"
    - "Generator returns answer with citation numbers [1], [2], etc."
    - "Generator handles low/no context gracefully with 'no answer found' response"
    - "Generator logs generation latency and token usage"
  artifacts:
    - path: "backend/app/services/generator.py"
      provides: "Answer generation service"
      exports: ["Generator"]
      min_lines: 80
  key_links:
    - from: "backend/app/services/generator.py"
      to: "OpenAI API"
      via: "AsyncOpenAI"
      pattern: "chat.completions.create"
    - from: "backend/app/services/generator.py"
      to: "backend/app/config.py"
      via: "settings.LLM_MODEL"
      pattern: "LLM_MODEL|OPENAI_API_KEY"
---

<objective>
Implement answer generation service using OpenAI GPT-5-mini.

Purpose: Generate grounded answers from retrieved chunks with proper citations and technical accuracy.
Output: Generator service that builds prompts, calls GPT-5-mini, and returns answers with citations
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-RESEARCH.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-CONTEXT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-01-SUMMARY.md
@backend/app/config.py
@backend/app/models/chat.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Generator service</name>
  <files>backend/app/services/generator.py, backend/app/services/__init__.py</files>
  <action>
Create backend/app/services/generator.py:

```python
"""Answer generation service using OpenAI GPT-5-mini."""
import time
from typing import List, Dict, Any, Optional
from openai import AsyncOpenAI
from app.config import settings
from app.models.chat import Citation
from app.core.logging import get_logger

logger = get_logger()


# System prompt for technical documentation assistant
SYSTEM_PROMPT = """You are a technical documentation assistant for aerospace/defense systems.
Answer questions using ONLY the provided document excerpts.
Include citation numbers [1], [2], etc. for each claim you make.
If information is not in the documents, say "Based on the available documents, I cannot find information about X."
Use concise, technical language (2-4 sentences).
When sources conflict, acknowledge the disagreement with citations.
Use Markdown for code snippets, formulas, or structured data."""


class Generator:
    """
    Answer generation using OpenAI GPT-5-mini.

    Builds grounded prompts from retrieved chunks and generates
    answers with inline citations.
    """

    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.LLM_MODEL
        self.temperature = settings.LLM_TEMPERATURE
        self.max_tokens = settings.LLM_MAX_TOKENS

    async def generate(
        self,
        query: str,
        chunks: List[Dict[str, Any]],
        request_id: str,
        history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        Generate answer from retrieved chunks.

        Args:
            query: User's question
            chunks: Reranked chunks with metadata
            request_id: Request correlation ID
            history: Optional conversation history

        Returns:
            {
                "answer": Generated answer text,
                "citations": List of Citation objects,
                "latency_ms": Generation time in ms,
                "tokens_used": Token count for diagnostics
            }
        """
```

The generate method should:

1. **Handle empty/low context:**
   - If no chunks: return "I cannot find relevant information in the uploaded documents."
   - Log warning with request_id

2. **Build context from chunks:**
   ```python
   context_parts = []
   for idx, chunk in enumerate(chunks, 1):
       citation_header = f"[{idx}: {chunk['filename']}, p.{chunk['page_range']}]"
       context_parts.append(f"{citation_header}\n{chunk['text']}\n")
   context = "\n".join(context_parts)
   ```

3. **Build user prompt:**
   ```
   Context:
   {context}

   Question: {query}

   Answer the question using only the context above. Include citation numbers [1], [2], etc.
   ```

4. **Include conversation history if provided:**
   - Prepend history messages to the messages list
   - Keep only last 5 turns to manage context window

5. **Call OpenAI API:**
   ```python
   response = await self.client.chat.completions.create(
       model=self.model,
       messages=[...],
       temperature=self.temperature,
       max_tokens=self.max_tokens
   )
   ```

6. **Build Citation objects:**
   - Create Citation for each chunk used in context
   - Include doc_id, filename, page_range, section_title, snippet (first 200 chars)
   - Include rerank_score if available

7. **Log diagnostics:**
   - Generation latency
   - Tokens used (prompt + completion)
   - Request ID for correlation

**Error handling:**
- If API key invalid: raise clear error (don't hide)
- If API call fails: log error, raise HTTPException with user-friendly message
- Timeout handling: 30s timeout for generation

Update backend/app/services/__init__.py to export Generator.
  </action>
  <verify>
python -c "from app.services.generator import Generator, SYSTEM_PROMPT; print('Generator loaded, prompt length:', len(SYSTEM_PROMPT))"
  </verify>
  <done>
Generator.generate() builds grounded prompts and returns answers with citations
  </done>
</task>

<task type="auto">
  <name>Task 2: Add generator tests</name>
  <files>backend/tests/test_generator.py</files>
  <action>
Create backend/tests/test_generator.py with unit tests:

```python
"""Tests for generator service."""
import pytest
from unittest.mock import patch, AsyncMock, MagicMock
from app.services.generator import Generator, SYSTEM_PROMPT


class TestGenerator:
    """Test generator service."""

    def test_system_prompt_content(self):
        """Test system prompt includes grounding instructions."""
        assert "ONLY" in SYSTEM_PROMPT
        assert "citation" in SYSTEM_PROMPT.lower()
        assert "[1]" in SYSTEM_PROMPT or "citation numbers" in SYSTEM_PROMPT

    @pytest.mark.asyncio
    async def test_generate_empty_chunks(self):
        """Test generation with no chunks returns 'no info' message."""
        generator = Generator()

        with patch.object(generator, 'client') as mock_client:
            result = await generator.generate(
                query="test question",
                chunks=[],
                request_id="test-123"
            )

        assert "cannot find" in result["answer"].lower()
        assert result["citations"] == []

    @pytest.mark.asyncio
    @patch('app.services.generator.AsyncOpenAI')
    async def test_generate_builds_context(self, mock_openai):
        """Test context building from chunks."""
        # Setup mock
        mock_client = AsyncMock()
        mock_openai.return_value = mock_client
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Answer [1]."))]
        mock_response.usage = MagicMock(total_tokens=100)
        mock_client.chat.completions.create = AsyncMock(return_value=mock_response)

        generator = Generator()
        generator.client = mock_client

        chunks = [
            {
                "text": "Test content here",
                "doc_id": "doc-1",
                "filename": "test.pdf",
                "page_range": "1-2",
                "section_title": "Introduction",
                "rerank_score": 0.9
            }
        ]

        result = await generator.generate(
            query="What is the test about?",
            chunks=chunks,
            request_id="test-123"
        )

        # Verify context was built
        call_args = mock_client.chat.completions.create.call_args
        messages = call_args.kwargs["messages"]

        # Should have system + user message
        assert len(messages) >= 2
        # User message should contain context
        user_msg = [m for m in messages if m["role"] == "user"][0]
        assert "[1:" in user_msg["content"]
        assert "test.pdf" in user_msg["content"]

    @pytest.mark.asyncio
    async def test_generate_builds_citations(self):
        """Test citation objects are built correctly."""
        generator = Generator()

        chunks = [
            {
                "text": "Content " * 50,  # Long content for snippet test
                "doc_id": "doc-1",
                "filename": "manual.pdf",
                "page_range": "42-43",
                "section_title": "Config",
                "rerank_score": 0.85
            }
        ]

        with patch.object(generator.client.chat.completions, 'create') as mock_create:
            mock_response = MagicMock()
            mock_response.choices = [MagicMock(message=MagicMock(content="Test [1]."))]
            mock_response.usage = MagicMock(total_tokens=50)
            mock_create.return_value = mock_response

            result = await generator.generate(
                query="test",
                chunks=chunks,
                request_id="test-123"
            )

        assert len(result["citations"]) == 1
        citation = result["citations"][0]
        assert citation.id == 1
        assert citation.doc_id == "doc-1"
        assert citation.filename == "manual.pdf"
        assert citation.page_range == "42-43"
        assert len(citation.snippet) <= 203  # 200 + "..."
```

Note: Full integration test with real API requires OPENAI_API_KEY.
  </action>
  <verify>
cd c:/Projects/ironmind/backend && python -m pytest tests/test_generator.py -v --tb=short 2>/dev/null || echo "Tests created"
  </verify>
  <done>
Generator tests exist covering system prompt, empty chunks, context building, and citations
  </done>
</task>

</tasks>

<verification>
1. Generator imports without error: `python -c "from app.services.generator import Generator"`
2. SYSTEM_PROMPT includes grounding instructions
3. Generator handles empty chunks gracefully
4. Tests pass: `pytest tests/test_generator.py`
</verification>

<success_criteria>
- Generator class initializes with OpenAI client and settings
- generate() method accepts query, chunks, request_id, history
- generate() returns dict with answer, citations, latency_ms, tokens_used
- Empty chunks return "cannot find information" message
- Context built with numbered citations [1: filename, p.X]
- Citation objects include id, doc_id, filename, page_range, snippet
- SYSTEM_PROMPT enforces grounding (ONLY from documents)
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-rag-with-hybrid-retrieval/03-04-SUMMARY.md`
</output>
