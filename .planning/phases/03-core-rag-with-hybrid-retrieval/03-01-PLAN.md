---
phase: 03-core-rag-with-hybrid-retrieval
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/config.py
  - backend/app/models/chat.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Settings load without errors when environment variables are present"
    - "Settings validate required API keys at startup"
    - "Chat requests validate question length (1-2000 chars) and require user_id"
    - "Chat responses include answer, citations list, and request_id"
    - "Citation objects capture doc_id, filename, page_range, and snippet for traceability"
  artifacts:
    - path: "backend/app/config.py"
      provides: "RAG pipeline configuration"
      contains: "OPENAI_API_KEY"
    - path: "backend/app/models/chat.py"
      provides: "Chat data models"
      exports: ["ChatRequest", "ChatResponse", "Citation"]
    - path: "backend/requirements.txt"
      provides: "Python dependencies"
      contains: "litellm"
  key_links:
    - from: "backend/app/config.py"
      to: "environment variables"
      via: "pydantic-settings"
      pattern: "OPENAI_API_KEY|DEEPINFRA_API_KEY|LLM_MODEL"
---

<objective>
Create configuration and data models for RAG pipeline.

Purpose: Establish settings for OpenAI embeddings, DeepInfra reranker, and GPT-5-mini LLM, plus request/response schemas for chat endpoint.
Output: Extended config.py with RAG settings, new chat.py models, updated requirements.txt
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-RESEARCH.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-CONTEXT.md
@backend/app/config.py
@backend/app/models/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend configuration for RAG pipeline</name>
  <files>backend/app/config.py, backend/requirements.txt</files>
  <action>
Update backend/app/config.py Settings class with RAG pipeline configuration:

**Add settings fields:**
- OPENAI_API_KEY: str (required for embeddings + LLM)
- OPENAI_EMBEDDING_MODEL: str = "text-embedding-3-small"
- DEEPINFRA_API_KEY: str (required for reranker)
- RERANK_MODEL: str = "Qwen/Qwen3-Reranker-0.6B" (DeepInfra model path)
- LLM_MODEL: str = "gpt-5-mini"
- LLM_TEMPERATURE: float = 0.1 (low for factual accuracy)
- LLM_MAX_TOKENS: int = 500

**Hybrid search settings:**
- HYBRID_WEIGHT: float = 0.5 (50/50 semantic/BM25)
- RETRIEVAL_LIMIT: int = 25 (initial retrieval count)
- RERANK_LIMIT: int = 12 (chunks to reranker)
- CONTEXT_LIMIT: int = 10 (chunks to LLM)
- RELEVANCE_THRESHOLD: float = 0.3 (minimum score to include)

**Cache settings:**
- CACHE_TTL_SECONDS: int = 300 (5 min default)

Update backend/requirements.txt to add:
- litellm>=1.0.0 (for DeepInfra reranker API)
- openai>=1.0.0 (for embeddings + LLM)
- redis>=5.0.0 (optional, for caching)

Note: Keep extra='ignore' in Config class for forward compatibility.
  </action>
  <verify>
python -c "from app.config import settings; print(settings.OPENAI_EMBEDDING_MODEL, settings.RERANK_MODEL, settings.LLM_MODEL)"
  </verify>
  <done>
Settings loads with all RAG configuration fields and sensible defaults
  </done>
</task>

<task type="auto">
  <name>Task 2: Create chat data models</name>
  <files>backend/app/models/chat.py, backend/app/models/__init__.py</files>
  <action>
Create backend/app/models/chat.py with Pydantic models:

**Citation model:**
```python
class Citation(BaseModel):
    id: int  # Footnote number [1], [2], etc.
    doc_id: str  # Document UUID
    filename: str  # Original filename
    page_range: str  # e.g., "42-43"
    section_title: Optional[str] = None
    snippet: str  # First 200 chars of chunk text
    score: Optional[float] = None  # Reranker score for diagnostics
```

**ChatRequest model:**
```python
class ChatRequest(BaseModel):
    question: str = Field(..., min_length=1, max_length=2000, description="User's question")
    user_id: str = Field(..., description="User ID for document filtering")
    history: Optional[List[dict]] = Field(default=None, description="Conversation history")
```

**DiagnosticInfo model (for observability):**
```python
class DiagnosticInfo(BaseModel):
    retrieval_count: int
    rerank_count: int
    context_count: int
    retrieval_latency_ms: int
    rerank_latency_ms: int
    generation_latency_ms: int
    total_latency_ms: int
    cache_hit: bool = False
```

**ChatResponse model:**
```python
class ChatResponse(BaseModel):
    answer: str
    citations: List[Citation]
    request_id: str
    diagnostics: Optional[DiagnosticInfo] = None  # Included in debug mode
```

Update backend/app/models/__init__.py to export chat models.
  </action>
  <verify>
python -c "from app.models.chat import ChatRequest, ChatResponse, Citation, DiagnosticInfo; print('Models loaded')"
  </verify>
  <done>
Chat models import successfully with proper validation
  </done>
</task>

</tasks>

<verification>
1. Settings loads all RAG configuration: `python -c "from app.config import settings; print(vars(settings))"`
2. Chat models validate correctly: `python -c "from app.models.chat import ChatRequest; r = ChatRequest(question='test', user_id='u1'); print(r.model_dump())"`
3. Requirements include new dependencies: `grep -E 'litellm|openai|redis' backend/requirements.txt`
</verification>

<success_criteria>
- Settings.OPENAI_API_KEY, DEEPINFRA_API_KEY fields exist (may be empty without env vars)
- Settings.OPENAI_EMBEDDING_MODEL defaults to "text-embedding-3-small"
- Settings.RERANK_MODEL defaults to "Qwen/Qwen3-Reranker-0.6B"
- Settings.LLM_MODEL defaults to "gpt-5-mini"
- ChatRequest validates question (1-2000 chars) and user_id (required)
- ChatResponse includes answer, citations list, request_id
- Citation includes doc_id, filename, page_range, snippet
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-rag-with-hybrid-retrieval/03-01-SUMMARY.md`
</output>
