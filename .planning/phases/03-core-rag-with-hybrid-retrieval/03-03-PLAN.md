---
phase: 03-core-rag-with-hybrid-retrieval
plan: 03
type: execute
wave: 2
depends_on: ["03-01"]
files_modified:
  - backend/app/services/reranker.py
autonomous: true
user_setup:
  - service: deepinfra
    why: "Reranking API for precision boost"
    env_vars:
      - name: DEEPINFRA_API_KEY
        source: "DeepInfra Dashboard -> API Keys (https://deepinfra.com/dash/api_keys)"

must_haves:
  truths:
    - "Reranker calls DeepInfra API with Qwen3-Reranker model"
    - "Reranker accepts query + list of chunks and returns reranked list"
    - "Reranker scores are attached to chunks for diagnostics"
    - "Reranker handles API errors gracefully with fallback to input order"
    - "Reranker logs latency and score distribution"
  artifacts:
    - path: "backend/app/services/reranker.py"
      provides: "Reranking service"
      exports: ["Reranker"]
      min_lines: 60
  key_links:
    - from: "backend/app/services/reranker.py"
      to: "DeepInfra API"
      via: "litellm.rerank"
      pattern: "rerank"
    - from: "backend/app/services/reranker.py"
      to: "backend/app/config.py"
      via: "settings.RERANK_MODEL"
      pattern: "RERANK_MODEL|DEEPINFRA_API_KEY"
---

<objective>
Implement reranker service using DeepInfra Qwen3-Reranker.

Purpose: Apply cross-encoder reranking to improve retrieval precision (30-50% boost) before sending chunks to LLM.
Output: Reranker service that calls DeepInfra API and returns reranked chunks with scores
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-RESEARCH.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-CONTEXT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-01-SUMMARY.md
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Reranker service</name>
  <files>backend/app/services/reranker.py, backend/app/services/__init__.py</files>
  <action>
Create backend/app/services/reranker.py:

```python
"""Reranker service using DeepInfra Qwen3-Reranker."""
import os
import time
from typing import List, Dict, Any
from litellm import rerank
from app.config import settings
from app.core.logging import get_logger

logger = get_logger()


class Reranker:
    """
    Reranker using DeepInfra Qwen3-Reranker model.

    Note: Research confirmed Mistral does NOT have a reranking model.
    DeepInfra provides Qwen3-Reranker variants (0.6B, 4B, 8B).
    Using 0.6B for speed (1-2s latency target).
    """

    def __init__(self):
        # Set DeepInfra API key for litellm
        os.environ["DEEPINFRA_API_KEY"] = settings.DEEPINFRA_API_KEY
        self.model = f"deepinfra/{settings.RERANK_MODEL}"

    async def rerank(
        self,
        query: str,
        chunks: List[Dict[str, Any]],
        request_id: str,
        top_k: int = None
    ) -> Dict[str, Any]:
        """
        Rerank chunks using cross-encoder model.

        Args:
            query: User's question
            chunks: List of chunk dicts with 'text' field
            request_id: Request correlation ID
            top_k: Return top K chunks (default: settings.RERANK_LIMIT)

        Returns:
            {
                "chunks": Reranked list with rerank_score added,
                "count": Number of chunks returned,
                "latency_ms": Reranking time in ms
            }
        """
```

The rerank method should:
1. Extract text from chunks for reranker input
2. Call litellm.rerank() with model, query, documents, top_n
3. Map reranker results back to original chunks (preserve all metadata)
4. Add rerank_score and rerank_rank to each chunk
5. Log reranking diagnostics (latency, score distribution)
6. Handle errors gracefully: if API fails, log warning and return original order

**Error handling:**
- If DEEPINFRA_API_KEY not set: log error, return chunks unchanged (skip reranking)
- If API call fails: log error with request_id, return chunks unchanged
- Never crash the pipeline on reranker failure

Update backend/app/services/__init__.py to export Reranker.
  </action>
  <verify>
python -c "from app.services.reranker import Reranker; r = Reranker(); print('Reranker loaded, model:', r.model)"
  </verify>
  <done>
Reranker.rerank() calls DeepInfra API and returns reranked chunks with scores
  </done>
</task>

<task type="auto">
  <name>Task 2: Add reranker integration test</name>
  <files>backend/tests/test_reranker.py</files>
  <action>
Create backend/tests/test_reranker.py with unit tests:

```python
"""Tests for reranker service."""
import pytest
from unittest.mock import patch, AsyncMock
from app.services.reranker import Reranker


class TestReranker:
    """Test reranker service."""

    def test_reranker_init(self):
        """Test reranker initialization."""
        reranker = Reranker()
        assert "deepinfra" in reranker.model
        assert "Qwen" in reranker.model

    @pytest.mark.asyncio
    async def test_rerank_empty_chunks(self):
        """Test reranking empty list returns empty."""
        reranker = Reranker()
        result = await reranker.rerank(
            query="test query",
            chunks=[],
            request_id="test-123"
        )
        assert result["chunks"] == []
        assert result["count"] == 0

    @pytest.mark.asyncio
    @patch('app.services.reranker.rerank')
    async def test_rerank_success(self, mock_rerank):
        """Test successful reranking."""
        # Mock litellm rerank response
        mock_rerank.return_value = AsyncMock()
        mock_rerank.return_value.results = [
            type('Result', (), {'index': 1, 'relevance_score': 0.9})(),
            type('Result', (), {'index': 0, 'relevance_score': 0.7})(),
        ]

        reranker = Reranker()
        chunks = [
            {"text": "chunk 1 content", "doc_id": "d1"},
            {"text": "chunk 2 content", "doc_id": "d2"},
        ]

        result = await reranker.rerank(
            query="test query",
            chunks=chunks,
            request_id="test-123",
            top_k=2
        )

        assert result["count"] == 2
        # Chunk 2 should be first (higher score)
        assert result["chunks"][0]["doc_id"] == "d2"
        assert result["chunks"][0]["rerank_score"] == 0.9

    @pytest.mark.asyncio
    async def test_rerank_api_error_fallback(self):
        """Test graceful fallback on API error."""
        # This tests actual behavior without mocking
        # If API key invalid, should return original order
        reranker = Reranker()
        chunks = [{"text": "chunk 1", "doc_id": "d1"}]

        # With invalid/missing API key, should not crash
        result = await reranker.rerank(
            query="test",
            chunks=chunks,
            request_id="test-123"
        )

        # Should return original chunks on error
        assert len(result["chunks"]) == 1
```

Note: Full integration test with real API requires DEEPINFRA_API_KEY in environment.
Mark those tests with @pytest.mark.integration for separate test run.
  </action>
  <verify>
cd c:/Projects/ironmind/backend && python -m pytest tests/test_reranker.py -v --tb=short 2>/dev/null || echo "Tests created (may fail without API key)"
  </verify>
  <done>
Reranker tests exist covering init, empty input, success mock, and error fallback
  </done>
</task>

</tasks>

<verification>
1. Reranker imports without error: `python -c "from app.services.reranker import Reranker"`
2. Reranker uses correct model path: Should include "deepinfra" and "Qwen"
3. Error handling works: Reranker.rerank() with invalid API key returns original chunks (doesn't crash)
4. Tests pass (unit tests, not integration): `pytest tests/test_reranker.py`
</verification>

<success_criteria>
- Reranker class initializes with DeepInfra model path
- rerank() method accepts query, chunks, request_id, top_k
- rerank() returns dict with chunks, count, latency_ms
- Each reranked chunk has rerank_score and rerank_rank fields
- API errors handled gracefully (return original order, log warning)
- Missing API key handled gracefully (skip reranking, log error)
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-rag-with-hybrid-retrieval/03-03-SUMMARY.md`
</output>
