---
phase: 03-core-rag-with-hybrid-retrieval
plan: 05
type: execute
wave: 3
depends_on: ["03-02", "03-03", "03-04"]
files_modified:
  - backend/app/routers/chat.py
  - backend/app/main.py
autonomous: false

must_haves:
  truths:
    - "POST /api/chat accepts question and user_id"
    - "Endpoint executes full RAG pipeline: retrieve -> rerank -> generate"
    - "Response includes answer with citation numbers [1], [2]"
    - "Response includes citations list with doc_id, filename, page_range, snippet"
    - "Pipeline completes in under 10 seconds for typical queries"
    - "System logs request_id, latencies, and diagnostic scores"
  artifacts:
    - path: "backend/app/routers/chat.py"
      provides: "Chat API endpoint"
      exports: ["router"]
      min_lines: 60
    - path: "backend/app/main.py"
      provides: "Router registration"
      contains: "chat.router"
  key_links:
    - from: "backend/app/routers/chat.py"
      to: "backend/app/services/retriever.py"
      via: "HybridRetriever.retrieve"
      pattern: "retriever.retrieve"
    - from: "backend/app/routers/chat.py"
      to: "backend/app/services/reranker.py"
      via: "Reranker.rerank"
      pattern: "reranker.rerank"
    - from: "backend/app/routers/chat.py"
      to: "backend/app/services/generator.py"
      via: "Generator.generate"
      pattern: "generator.generate"
---

<objective>
Implement POST /api/chat endpoint with full RAG pipeline orchestration.

Purpose: Expose the complete RAG pipeline (retrieve -> rerank -> generate) as an authenticated API endpoint.
Output: Chat router with pipeline orchestration, diagnostics logging, and end-to-end verification
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-RESEARCH.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-CONTEXT.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-02-SUMMARY.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-03-SUMMARY.md
@.planning/phases/03-core-rag-with-hybrid-retrieval/03-04-SUMMARY.md
@backend/app/routers/documents.py
@backend/app/main.py
@backend/app/models/chat.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat router with RAG pipeline</name>
  <files>backend/app/routers/chat.py, backend/app/main.py</files>
  <action>
Create backend/app/routers/chat.py:

```python
"""Chat API endpoint with RAG pipeline."""
import time
import uuid
from fastapi import APIRouter, HTTPException, Depends
from app.models.chat import ChatRequest, ChatResponse, DiagnosticInfo
from app.services.retriever import HybridRetriever
from app.services.reranker import Reranker
from app.services.generator import Generator
from app.middleware.auth import get_current_user
from app.core.logging import get_logger
from app.config import settings

router = APIRouter(prefix="/api", tags=["chat"])
logger = get_logger()

# Initialize services (singleton pattern)
retriever = HybridRetriever()
reranker = Reranker()
generator = Generator()


@router.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    RAG chat endpoint with hybrid retrieval, reranking, and generation.

    Three-stage pipeline:
    1. Hybrid Retrieval: semantic + BM25 search (25 chunks)
    2. Reranking: Cross-encoder scoring (top 12)
    3. Generation: GPT-5-mini with grounding (top 10)

    Performance target: <10 seconds (typical: 5-8s)
    """
    request_id = str(uuid.uuid4())
    start_time = time.time()

    # Use authenticated user_id (override request.user_id for security)
    user_id = current_user.get("user_id", request.user_id)

    logger.info("chat_request_received",
                request_id=request_id,
                user_id=user_id,
                query_length=len(request.question))

    try:
        # Stage 1: Hybrid Retrieval
        retrieval_result = await retriever.retrieve(
            query=request.question,
            user_id=user_id,
            request_id=request_id
        )

        if retrieval_result["count"] == 0:
            # No relevant documents found
            return ChatResponse(
                answer="I couldn't find relevant information in your uploaded documents. Please try rephrasing your question or ensure relevant documents are uploaded.",
                citations=[],
                request_id=request_id,
                diagnostics=DiagnosticInfo(
                    retrieval_count=0,
                    rerank_count=0,
                    context_count=0,
                    retrieval_latency_ms=retrieval_result["latency_ms"],
                    rerank_latency_ms=0,
                    generation_latency_ms=0,
                    total_latency_ms=int((time.time() - start_time) * 1000)
                )
            )

        # Stage 2: Reranking
        rerank_result = await reranker.rerank(
            query=request.question,
            chunks=retrieval_result["chunks"],
            request_id=request_id
        )

        # Stage 3: Generation
        context_chunks = rerank_result["chunks"][:settings.CONTEXT_LIMIT]
        generation_result = await generator.generate(
            query=request.question,
            chunks=context_chunks,
            request_id=request_id,
            history=request.history
        )

        total_latency_ms = int((time.time() - start_time) * 1000)

        # Build diagnostic info
        diagnostics = DiagnosticInfo(
            retrieval_count=retrieval_result["count"],
            rerank_count=rerank_result["count"],
            context_count=len(context_chunks),
            retrieval_latency_ms=retrieval_result["latency_ms"],
            rerank_latency_ms=rerank_result["latency_ms"],
            generation_latency_ms=generation_result["latency_ms"],
            total_latency_ms=total_latency_ms
        )

        logger.info("chat_request_complete",
                    request_id=request_id,
                    total_latency_ms=total_latency_ms,
                    retrieval_count=retrieval_result["count"],
                    citations_count=len(generation_result["citations"]))

        return ChatResponse(
            answer=generation_result["answer"],
            citations=generation_result["citations"],
            request_id=request_id,
            diagnostics=diagnostics
        )

    except Exception as e:
        logger.error("chat_request_failed",
                     request_id=request_id,
                     error=str(e),
                     exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Chat processing failed. Request ID: {request_id}"
        )
```

Update backend/app/main.py to register chat router:
```python
from app.routers import chat
# ...
app.include_router(chat.router)
```
  </action>
  <verify>
python -c "from app.routers.chat import router; print('Chat router loaded, routes:', [r.path for r in router.routes])"
  </verify>
  <done>
POST /api/chat endpoint registered and orchestrates full RAG pipeline
  </done>
</task>

<task type="auto">
  <name>Task 2: Add chat endpoint integration test</name>
  <files>backend/tests/test_chat_endpoint.py</files>
  <action>
Create backend/tests/test_chat_endpoint.py with tests:

```python
"""Integration tests for chat endpoint."""
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, AsyncMock
from app.main import app


class TestChatEndpoint:
    """Test chat API endpoint."""

    @pytest.fixture
    def client(self):
        """Create test client."""
        return TestClient(app)

    @pytest.fixture
    def mock_auth(self):
        """Mock authentication."""
        with patch('app.routers.chat.get_current_user') as mock:
            mock.return_value = {"user_id": "test-user-123"}
            yield mock

    def test_chat_requires_auth(self, client):
        """Test chat endpoint requires authentication."""
        response = client.post("/api/chat", json={
            "question": "test",
            "user_id": "user-1"
        })
        assert response.status_code == 401

    @pytest.mark.asyncio
    async def test_chat_empty_results(self, client, mock_auth):
        """Test chat with no matching documents."""
        with patch('app.routers.chat.retriever') as mock_retriever:
            mock_retriever.retrieve = AsyncMock(return_value={
                "chunks": [],
                "count": 0,
                "latency_ms": 100
            })

            response = client.post("/api/chat", json={
                "question": "What is the weather?",
                "user_id": "test-user-123"
            })

            assert response.status_code == 200
            data = response.json()
            assert "couldn't find" in data["answer"].lower()
            assert data["citations"] == []
            assert data["diagnostics"]["retrieval_count"] == 0

    @pytest.mark.asyncio
    async def test_chat_full_pipeline(self, client, mock_auth):
        """Test full RAG pipeline execution."""
        with patch('app.routers.chat.retriever') as mock_retriever, \
             patch('app.routers.chat.reranker') as mock_reranker, \
             patch('app.routers.chat.generator') as mock_generator:

            # Mock retrieval
            mock_retriever.retrieve = AsyncMock(return_value={
                "chunks": [
                    {"text": "chunk 1", "doc_id": "d1", "filename": "test.pdf", "page_range": "1-2"}
                ],
                "count": 1,
                "latency_ms": 200
            })

            # Mock reranking
            mock_reranker.rerank = AsyncMock(return_value={
                "chunks": [
                    {"text": "chunk 1", "doc_id": "d1", "filename": "test.pdf", "page_range": "1-2", "rerank_score": 0.9}
                ],
                "count": 1,
                "latency_ms": 300
            })

            # Mock generation
            mock_generator.generate = AsyncMock(return_value={
                "answer": "Test answer [1].",
                "citations": [
                    {"id": 1, "doc_id": "d1", "filename": "test.pdf", "page_range": "1-2", "snippet": "chunk 1"}
                ],
                "latency_ms": 500
            })

            response = client.post("/api/chat", json={
                "question": "What is in the document?",
                "user_id": "test-user-123"
            })

            assert response.status_code == 200
            data = response.json()
            assert data["answer"] == "Test answer [1]."
            assert len(data["citations"]) == 1
            assert data["diagnostics"]["retrieval_count"] == 1
            assert data["diagnostics"]["total_latency_ms"] > 0

    def test_chat_request_validation(self, client, mock_auth):
        """Test request validation."""
        # Empty question
        response = client.post("/api/chat", json={
            "question": "",
            "user_id": "user-1"
        })
        assert response.status_code == 422  # Validation error

        # Missing user_id
        response = client.post("/api/chat", json={
            "question": "test question"
        })
        assert response.status_code == 422
```
  </action>
  <verify>
cd c:/Projects/ironmind/backend && python -m pytest tests/test_chat_endpoint.py -v --tb=short 2>/dev/null || echo "Tests created"
  </verify>
  <done>
Chat endpoint tests exist covering auth, empty results, full pipeline, and validation
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>
Complete RAG pipeline with:
- POST /api/chat endpoint
- Three-stage pipeline: retrieve -> rerank -> generate
- Full observability (request_id, latencies, diagnostic scores)
- Citation generation with doc_id, filename, page_range, snippet
  </what-built>
  <how-to-verify>
1. Ensure services are running: `docker compose up -d`
2. Upload a test document via POST /api/documents/upload (from Phase 2)
3. Wait for document to be indexed (check status via GET /api/documents)
4. Test chat endpoint:
```bash
curl -X POST http://localhost:8000/api/chat \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"question": "What is described in the document?", "user_id": "YOUR_USER_ID"}'
```

5. Verify response contains:
   - answer: Non-empty text with citation numbers [1], [2]
   - citations: Array with doc_id, filename, page_range, snippet
   - request_id: UUID
   - diagnostics: latencies and counts

6. Check backend logs for:
   - chat_request_received event
   - retrieval, rerank, generation stages logged
   - chat_request_complete with total_latency_ms

7. Verify performance: Total latency should be under 10 seconds
  </how-to-verify>
  <resume-signal>Type "approved" if pipeline works end-to-end with citations, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
1. Chat router imports: `python -c "from app.routers.chat import router"`
2. Router registered in main.py: Check for `chat.router` include
3. POST /api/chat accessible (requires auth)
4. Full pipeline executes in under 10 seconds
5. Response includes answer, citations, request_id, diagnostics
</verification>

<success_criteria>
- POST /api/chat returns 401 without auth
- POST /api/chat returns 200 with valid auth and question
- Response has ChatResponse schema (answer, citations, request_id, diagnostics)
- Pipeline stages logged with request_id correlation
- Total latency logged and under 10 seconds
- Citations include doc_id, filename, page_range, snippet
- Empty retrieval results handled gracefully (user-friendly message)
</success_criteria>

<output>
After completion, create `.planning/phases/03-core-rag-with-hybrid-retrieval/03-05-SUMMARY.md`
</output>
