---
phase: 04-knowledge-graph-integration
plan: 03
type: execute
wave: 3
depends_on: ["04-01", "04-02"]
files_modified:
  - backend/app/services/pipeline.py
  - backend/app/models/documents.py
autonomous: true

must_haves:
  truths:
    - "Document ingestion extracts entities and stores them in FalkorDB"
    - "Graph extraction runs after chunking, before indexing completion"
    - "Pipeline status includes graph extraction stage"
  artifacts:
    - path: "backend/app/services/pipeline.py"
      provides: "Extended pipeline with graph extraction stage"
      contains: "GraphExtraction|EntityExtractor|graph_store"
    - path: "backend/app/models/documents.py"
      provides: "Updated ProcessingStatus with GRAPH_EXTRACTING stage"
      contains: "GRAPH_EXTRACTING"
  key_links:
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/graph/extractor.py"
      via: "EntityExtractor import and usage"
      pattern: "EntityExtractor|extractor\\.extract"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/graph/graph_store.py"
      via: "GraphStore import and entity storage"
      pattern: "GraphStore|graph_store\\.add_entity"
---

<objective>
Integrate graph extraction into the document processing pipeline.

Purpose: Extend the existing pipeline (parse -> chunk -> index) to include graph extraction as a new stage (parse -> chunk -> graph_extract -> index). Entities and relationships are extracted from chunks and stored in FalkorDB before indexing completes.

Output: Pipeline creates knowledge graph entries during document ingestion automatically.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-knowledge-graph-integration/04-CONTEXT.md
@.planning/phases/04-knowledge-graph-integration/04-01-SUMMARY.md
@.planning/phases/04-knowledge-graph-integration/04-02-SUMMARY.md
@backend/app/services/pipeline.py
@backend/app/models/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add GRAPH_EXTRACTING status and update stage weights</name>
  <files>backend/app/models/documents.py</files>
  <action>
    Update ProcessingStatus enum to include graph extraction stage:

    class ProcessingStatus(str, Enum):
        UPLOADING = "Uploading"
        PARSING = "Parsing"
        CHUNKING = "Chunking"
        GRAPH_EXTRACTING = "GraphExtracting"  # NEW - between chunking and indexing
        INDEXING = "Indexing"
        DONE = "Done"
        FAILED = "Failed"

    The new stage fits after CHUNKING and before INDEXING because:
    - We need chunks to extract entities from
    - Graph must be populated before retrieval can use it
  </action>
  <verify>
    python -c "
    from app.models.documents import ProcessingStatus
    assert ProcessingStatus.GRAPH_EXTRACTING.value == 'GraphExtracting'
    "
  </verify>
  <done>ProcessingStatus includes GRAPH_EXTRACTING stage for pipeline tracking.</done>
</task>

<task type="auto">
  <name>Task 2: Integrate graph extraction into pipeline</name>
  <files>backend/app/services/pipeline.py</files>
  <action>
    Modify DocumentPipeline class to include graph extraction:

    1. Add imports at top:
       from app.services.graph import EntityExtractor, GraphStore

    2. Update __init__ to initialize graph services:
       self.extractor = EntityExtractor()
       self.graph_store = GraphStore()

    3. Add new Stage 3.5: GRAPH_EXTRACTING in process_document() method:
       After chunking completes and before indexing:

       # Stage 3: GRAPH EXTRACTION (new)
       stage_start = time.time()
       await self._update_status(doc_id, user_id, ProcessingStatus.GRAPH_EXTRACTING, "Extracting entities for knowledge graph")

       entity_count = 0
       relationship_count = 0

       # First, clear any existing graph data for this document (for re-ingestion)
       self.graph_store.delete_document_entities(doc_id, user_id)

       # Extract from each chunk
       for chunk in chunks:
           extraction = await self.extractor.extract_from_chunk(
               chunk_text=chunk.text,
               doc_id=doc_id,
               chunk_id=chunk.chunk_id
           )

           # Store entities
           for entity in extraction.entities:
               self.graph_store.add_entity(entity, user_id)
               entity_count += 1

           # Store relationships
           for rel in extraction.relationships:
               self.graph_store.add_relationship(rel, user_id)
               relationship_count += 1

       processing_log.append(ProcessingLogEntry(
           stage="GraphExtracting",
           started_at=datetime.fromtimestamp(stage_start, timezone.utc),
           completed_at=datetime.now(timezone.utc),
           duration_ms=int((time.time() - stage_start) * 1000)
       ))

       logger.info("graph_extraction_completed",
                   doc_id=doc_id,
                   entity_count=entity_count,
                   relationship_count=relationship_count)

    4. Handle graph extraction errors gracefully:
       - Wrap in try/except
       - Log warning but continue to indexing (graph is enhancement, not critical path)
       - Don't fail entire pipeline if graph extraction fails

    5. Update doc.entity_count and doc.relationship_count in final DONE stage.
       (Add these fields to Document model if not present)
  </action>
  <verify>
    # Requires running services
    docker-compose up -d falkordb
    # Upload a test document and check logs for graph_extraction_completed event
  </verify>
  <done>Pipeline extracts entities from chunks and stores in FalkorDB during document ingestion.</done>
</task>

<task type="auto">
  <name>Task 3: Update stage weights for progress calculation</name>
  <files>backend/app/services/pipeline.py</files>
  <action>
    Update STAGE_WEIGHTS dict to include graph extraction:

    STAGE_WEIGHTS = {
        "Uploading": 0.10,
        "Parsing": 0.35,          # Reduced from 0.40
        "Chunking": 0.15,         # Reduced from 0.20
        "GraphExtracting": 0.15,  # NEW - graph extraction takes ~15% of time
        "Indexing": 0.25,         # Reduced from 0.30
        "Complete": 1.0
    }

    The weights should sum to 1.0 (excluding Complete).
    Graph extraction is estimated at 15% because:
    - Each chunk requires an LLM API call
    - But calls are batched and concurrent
    - Typical document: 4-5 chunks * ~500ms/call = 2-3s

    Update calculate_progress() function if needed to handle new stage.
  </action>
  <verify>
    python -c "
    from app.services.pipeline import STAGE_WEIGHTS
    weights = [v for k, v in STAGE_WEIGHTS.items() if k != 'Complete']
    assert abs(sum(weights) - 1.0) < 0.01, f'Weights sum to {sum(weights)}'
    print('Stage weights valid')
    "
  </verify>
  <done>Progress calculation includes graph extraction stage with 15% weight.</done>
</task>

</tasks>

<verification>
1. Status enum: ProcessingStatus.GRAPH_EXTRACTING exists
2. Pipeline integration: Upload document, check logs for graph_extraction_completed event
3. Graph populated: After ingestion, query FalkorDB for entities from document
4. Progress calculation: Status API returns correct progress during each stage
5. Re-ingestion: Upload same document twice, verify no duplicate entities (cleanup works)
6. Error resilience: Graph extraction failure doesn't crash pipeline
</verification>

<success_criteria>
- Document ingestion logs show graph_extraction_started and graph_extraction_completed events
- FalkorDB contains entities after document ingestion completes
- Each entity has correct user_id, doc_id, chunk_id for traceability
- Relationships link entities with context from source documents
- Pipeline continues to DONE even if graph extraction has partial failures
- Progress API returns appropriate percentage during GraphExtracting stage
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-graph-integration/04-03-SUMMARY.md`
</output>
