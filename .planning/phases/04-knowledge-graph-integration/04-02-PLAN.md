---
phase: 04-knowledge-graph-integration
plan: 02
type: execute
wave: 2
depends_on: ["04-01"]
files_modified:
  - backend/app/services/graph/extractor.py
  - backend/app/services/graph/__init__.py
autonomous: true

must_haves:
  truths:
    - "LLM extracts entities and relationships from chunk text"
    - "Extraction output conforms to Pydantic schema (100% compliance via Structured Outputs)"
    - "Entity resolution handles acronyms and disambiguates similar entities"
  artifacts:
    - path: "backend/app/services/graph/extractor.py"
      provides: "LLM-based entity/relationship extraction service"
      exports: ["EntityExtractor"]
    - path: "backend/app/services/graph/__init__.py"
      provides: "Updated exports"
      exports: ["Entity", "Relationship", "GraphExtraction", "EntityExtractor"]
  key_links:
    - from: "backend/app/services/graph/extractor.py"
      to: "openai"
      via: "AsyncOpenAI with Structured Outputs"
      pattern: "response_format.*GraphExtraction|parse_completion"
    - from: "backend/app/services/graph/extractor.py"
      to: "backend/app/services/graph/schemas.py"
      via: "Pydantic schema import"
      pattern: "from.*schemas import.*GraphExtraction"
---

<objective>
Implement LLM-based entity and relationship extraction using OpenAI Structured Outputs.

Purpose: Extract aerospace/defense entities (hardware, software, configuration, error) and their relationships from document chunks with 100% schema compliance. The extractor uses GPT-4o's Structured Outputs feature (not prompt engineering) to guarantee valid output.

Output: EntityExtractor service that transforms chunk text into typed Entity and Relationship objects.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-knowledge-graph-integration/04-CONTEXT.md
@.planning/phases/04-knowledge-graph-integration/04-RESEARCH.md
@.planning/phases/04-knowledge-graph-integration/04-01-SUMMARY.md
@backend/app/services/graph/schemas.py
@backend/app/services/retriever.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create extraction prompt and EntityExtractor class</name>
  <files>backend/app/services/graph/extractor.py</files>
  <action>
    Create EntityExtractor class with OpenAI Structured Outputs:

    1. Define EXTRACTION_PROMPT constant (system message):
       ```
       You are extracting entities and relationships from aerospace/defense technical documentation.

       ENTITIES - Extract ALL of these types:
       - hardware: Physical systems, subsystems, modules, sensors, actuators, components
       - software: APIs, services, protocols, algorithms, interfaces
       - configuration: Settings, thresholds, modes, flags, parameters
       - error: Error codes, fault conditions, failure scenarios, warning types

       For hierarchical entities, set parent_entity to the containing system.
       Example: "GPS Module" has parent_entity "Navigation System"

       Expand acronyms in descriptions (e.g., UAV = Unmanned Aerial Vehicle).
       Include singleton entities even if mentioned once.

       RELATIONSHIPS - Identify:
       - depends_on: X requires Y to function
       - configures: X sets parameters for Y
       - connects_to: X interfaces/communicates with Y
       - is_part_of: X is a component/subsystem of Y

       For each relationship, include the exact sentence from the document in the context field.
       ```

    2. __init__(self): Initialize AsyncOpenAI client with settings.OPENAI_API_KEY.
       Use model gpt-4o-2024-08-06 (supports Structured Outputs).

    3. async extract_from_chunk(self, chunk_text: str, doc_id: str, chunk_id: str) -> GraphExtraction:
       - Call client.beta.chat.completions.parse() with response_format=GraphExtraction
       - Pass chunk_text as user message
       - Set temperature=0 for deterministic extraction
       - Return parsed GraphExtraction object

       IMPORTANT: Use the beta.chat.completions.parse() method for Structured Outputs.
       The response_format parameter takes the Pydantic class directly.

    4. async extract_batch(self, chunks: List[dict], doc_id: str) -> List[GraphExtraction]:
       - Process multiple chunks concurrently using asyncio.gather()
       - Each chunk dict has: text, chunk_id
       - Return list of GraphExtraction results
       - Limit concurrency to 5 parallel requests (avoid rate limits)

    Handle OpenAI API errors gracefully:
    - Log error with chunk_id for debugging
    - Return empty GraphExtraction on failure (don't crash pipeline)

    Use structlog for logging extraction events.
  </action>
  <verify>
    export OPENAI_API_KEY=... (valid key)
    python -c "
    import asyncio
    from app.services.graph.extractor import EntityExtractor
    extractor = EntityExtractor()
    result = asyncio.run(extractor.extract_from_chunk(
        'The GPS Module connects to the Navigation System via serial interface.',
        'doc1', 'chunk1'
    ))
    print(f'Entities: {len(result.entities)}, Relationships: {len(result.relationships)}')
    "
  </verify>
  <done>EntityExtractor uses Structured Outputs to extract typed entities and relationships from chunk text.</done>
</task>

<task type="auto">
  <name>Task 2: Add entity resolution with acronym expansion</name>
  <files>backend/app/services/graph/extractor.py</files>
  <action>
    Extend EntityExtractor with entity resolution capabilities:

    1. Import ACRONYM_MAP from app.services.retriever (reuse aerospace acronym dictionary).
       Add any additional acronyms from CONTEXT.md not in current map.

    2. Add method normalize_entity_name(self, name: str) -> str:
       - Convert to title case
       - Expand known acronyms: "GPS" -> "Global Positioning System (GPS)"
       - Strip extra whitespace
       - This normalizes entity names for consistent matching

    3. Add async method resolve_entity(
         self,
         entity: Entity,
         existing_entities: List[str],
         context: str
       ) -> str:
       - If entity.name in existing_entities (exact match), return existing name
       - If normalized name matches existing, return existing name
       - For potential matches (acronym vs full form), use LLM disambiguation:
         "Is '{entity.name}' the same entity as any of these: {candidates}? Context: {context}"
       - Return canonical entity name (existing or new)

    4. Add method post_process_extraction(self, extraction: GraphExtraction, doc_id: str, chunk_id: str) -> GraphExtraction:
       - Normalize all entity names
       - Fill in doc_id and chunk_id for each entity (these aren't in LLM output)
       - Ensure relationship source/target match normalized entity names
       - Return cleaned GraphExtraction

    5. Update extract_from_chunk() to call post_process_extraction() before returning.

    Note: Full entity resolution across documents happens in GraphStore during ingestion.
    This method handles within-chunk normalization only.
  </action>
  <verify>
    python -c "
    from app.services.graph.extractor import EntityExtractor
    extractor = EntityExtractor()
    assert 'Global Positioning System' in extractor.normalize_entity_name('GPS')
    "
  </verify>
  <done>Entity names are normalized with acronym expansion, enabling consistent graph matching.</done>
</task>

<task type="auto">
  <name>Task 3: Update exports and add extraction metrics logging</name>
  <files>backend/app/services/graph/__init__.py, backend/app/services/graph/extractor.py</files>
  <action>
    1. Update __init__.py exports:
       from .schemas import Entity, Relationship, GraphExtraction
       from .extractor import EntityExtractor
       from .graph_store import GraphStore
       __all__ = ["Entity", "Relationship", "GraphExtraction", "EntityExtractor", "GraphStore"]

    2. Add extraction metrics logging to extract_from_chunk():
       - Log extraction_started with chunk_id, text_length
       - Log extraction_completed with entity_count, relationship_count, latency_ms
       - Log extraction_failed with error type on failure

    3. Add method get_extraction_stats(self) -> dict:
       - Track cumulative stats: total_extractions, total_entities, total_relationships, total_failures
       - Return as dict for diagnostics
       - Use instance variables to accumulate during batch processing
  </action>
  <verify>
    python -c "
    from app.services.graph import Entity, Relationship, GraphExtraction, EntityExtractor, GraphStore
    print('All exports available')
    "
  </verify>
  <done>Graph module exports all components, extraction includes metrics logging for quality monitoring.</done>
</task>

</tasks>

<verification>
1. Structured Output compliance: Extract from sample text, verify result is GraphExtraction type
2. Entity type enforcement: All extracted entities have type in [hardware, software, configuration, error]
3. Relationship type enforcement: All relationships have type in [depends_on, configures, connects_to, is_part_of]
4. Acronym expansion: "GPS Module" description includes "Global Positioning System"
5. Batch extraction: Process 3 chunks, verify 3 GraphExtraction results returned
6. Error handling: Pass empty string, verify empty GraphExtraction returned (no crash)
</verification>

<success_criteria>
- EntityExtractor.extract_from_chunk() returns valid GraphExtraction object
- All entities have type constrained to Literal types (no "component" or "system" types)
- All relationships have context field populated with source sentence
- Acronyms are expanded in entity descriptions
- Batch extraction handles 5+ chunks concurrently
- API errors logged but don't crash extraction pipeline
</success_criteria>

<output>
After completion, create `.planning/phases/04-knowledge-graph-integration/04-02-SUMMARY.md`
</output>
