---
phase: 05-multi-source-synthesis
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/app/services/generator.py
autonomous: true

must_haves:
  truths:
    - "Generator activates synthesis mode when chunks come from 2+ distinct documents"
    - "Synthesis prompt uses topic-organized structure with Chain-of-Thought"
    - "Generated answers use consensus language ('multiple sources mention', 'consistently')"
    - "Citations use compact notation [1-3] for 3+ sources on same claim"
  artifacts:
    - path: "backend/app/services/generator.py"
      provides: "Multi-source synthesis generation"
      contains: "SYNTHESIS_SYSTEM_PROMPT"
      contains: "should_activate_synthesis_mode"
  key_links:
    - from: "backend/app/services/generator.py"
      to: "backend/app/models/chat.py"
      via: "returns synthesis_mode and source_doc_count"
      pattern: "synthesis_mode.*True"
---

<objective>
Implement multi-source synthesis prompting in the answer generator.

Purpose: When answers involve 2+ documents, use topic-organized synthesis prompting with Chain-of-Thought to improve citation accuracy and handle consensus/conflicts.
Output: Enhanced Generator with synthesis mode detection and topic-organized prompting.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-source-synthesis/05-CONTEXT.md
@.planning/phases/05-multi-source-synthesis/05-RESEARCH.md
@.planning/phases/05-multi-source-synthesis/05-01-SUMMARY.md
@backend/app/services/generator.py
@backend/app/models/chat.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add synthesis mode detection and prompts</name>
  <files>backend/app/services/generator.py</files>
  <action>
Update generator.py with multi-source synthesis capabilities.

Add SYNTHESIS_SYSTEM_PROMPT constant after SYSTEM_PROMPT:

```python
# Multi-source synthesis prompt (activated when 2+ documents in context)
SYNTHESIS_SYSTEM_PROMPT = """You are a technical documentation assistant synthesizing information from multiple aerospace/defense sources.

When answering from multiple documents:
1. ORGANIZE by subtopics - group related information together
2. INDICATE CONSENSUS - use phrases like "multiple sources mention", "consistently described as", "according to multiple documents"
3. HANDLE CONFLICTS - when sources disagree, cite both perspectives with their citations
4. USE COMPACT CITATIONS - for 3+ sources on same point, use notation like [1-3] instead of [1][2][3]
5. PRESERVE TRACEABILITY - every claim needs citation support

Answer structure for multi-document questions:
- Brief overview (1 sentence)
- Subtopic 1: [information from sources] [citations]
- Subtopic 2: [information from sources] [citations]
- If conflicts exist: Note disagreements with citations

Use concise technical language (2-5 sentences per subtopic).
When citing knowledge graph information, note these are system-inferred relationships."""
```

Add helper function to detect synthesis mode:

```python
def should_activate_synthesis_mode(chunks: List[Dict[str, Any]]) -> bool:
    """
    Determine if multi-source synthesis mode should activate.

    Threshold: 2+ distinct documents in retrieved chunks, AND
    at least 2 chunks from each of 2 documents (avoid spurious triggers).
    """
    if not chunks:
        return False

    # Count chunks per document
    doc_chunk_counts = {}
    for chunk in chunks:
        doc_id = chunk.get('doc_id', '')
        # Skip graph-derived chunks for synthesis trigger
        if chunk.get('source') == 'graph':
            continue
        doc_chunk_counts[doc_id] = doc_chunk_counts.get(doc_id, 0) + 1

    # Require 2+ documents with 2+ chunks each
    multi_chunk_docs = sum(1 for count in doc_chunk_counts.values() if count >= 2)
    return multi_chunk_docs >= 2
```

Add helper function to build topic-organized context:

```python
def build_synthesis_context(chunks: List[Dict[str, Any]]) -> str:
    """
    Build context grouped by document for synthesis prompting.

    Groups chunks by source document to make cross-document patterns visible.
    """
    doc_groups = {}

    for idx, chunk in enumerate(chunks, 1):
        doc_id = chunk.get('doc_id', 'unknown')
        if doc_id not in doc_groups:
            doc_groups[doc_id] = {
                'filename': chunk.get('filename', 'Unknown'),
                'chunks': []
            }
        doc_groups[doc_id]['chunks'].append((idx, chunk))

    context_parts = []
    for doc_id, group in doc_groups.items():
        filename = group['filename']
        context_parts.append(f"\n=== {filename} ===")

        for idx, chunk in group['chunks']:
            page_range = chunk.get('page_range', '?')
            source_type = chunk.get('source', 'document')

            if source_type == 'graph':
                entity_name = chunk.get('entity_name', 'Unknown')
                header = f"[{idx}: Knowledge Graph - {entity_name}]"
            else:
                header = f"[{idx}: p.{page_range}]"

            context_parts.append(f"{header}\n{chunk.get('text', '')}\n")

    return "\n".join(context_parts)
```

Update the generate method to use synthesis mode. Replace the generate method with this enhanced version:

```python
async def generate(
    self,
    query: str,
    chunks: List[Dict[str, Any]],
    request_id: str,
    history: Optional[List[Dict]] = None
) -> Dict[str, Any]:
    """
    Generate answer from retrieved chunks.

    Activates synthesis mode when chunks come from 2+ distinct documents.

    Args:
        query: User's question
        chunks: Reranked chunks with metadata
        request_id: Request correlation ID
        history: Optional conversation history

    Returns:
        {
            "answer": Generated answer text,
            "citations": List of Citation objects,
            "latency_ms": Generation time in ms,
            "tokens_used": Token count for diagnostics,
            "synthesis_mode": Whether multi-doc synthesis was used,
            "source_doc_count": Number of distinct source documents
        }
    """
    start_time = time.time()

    # Handle empty/low context
    if not chunks:
        logger.warning(
            "no_chunks_for_generation",
            request_id=request_id,
            query=query[:100]
        )
        return {
            "answer": "I cannot find relevant information in the uploaded documents.",
            "citations": [],
            "latency_ms": int((time.time() - start_time) * 1000),
            "tokens_used": 0,
            "synthesis_mode": False,
            "source_doc_count": 0
        }

    # Detect synthesis mode
    synthesis_mode = should_activate_synthesis_mode(chunks)
    unique_doc_ids = set(c.get('doc_id') for c in chunks if c.get('source') != 'graph')
    source_doc_count = len(unique_doc_ids)

    # Choose prompt and context format based on mode
    if synthesis_mode:
        system_prompt = SYNTHESIS_SYSTEM_PROMPT
        context = build_synthesis_context(chunks)
        user_prompt = f"""Context from {source_doc_count} documents:
{context}

Question: {query}

Think step-by-step:
1. What are the main subtopics relevant to this question?
2. What does each document say about each subtopic?
3. Where do documents agree? Where do they differ?
4. Synthesize a topic-organized answer with citations.

Answer:"""
        logger.info(
            "synthesis_mode_activated",
            request_id=request_id,
            source_doc_count=source_doc_count
        )
    else:
        system_prompt = SYSTEM_PROMPT
        # Standard context format
        context_parts = []
        for idx, chunk in enumerate(chunks, 1):
            if chunk.get('source') == 'graph':
                entity_name = chunk.get('entity_name', 'Unknown')
                citation_header = f"[{idx}: Knowledge Graph - {entity_name}]"
            else:
                citation_header = f"[{idx}: {chunk['filename']}, p.{chunk['page_range']}]"
            context_parts.append(f"{citation_header}\n{chunk['text']}\n")
        context = "\n".join(context_parts)
        user_prompt = f"""Context:
{context}

Question: {query}

Answer the question using only the context above. Include citation numbers [1], [2], etc."""

    # Build messages list
    messages = [
        {"role": "system", "content": system_prompt},
    ]

    # Include conversation history if provided (keep last 5 turns)
    if history:
        recent_history = history[-10:]
        messages.extend(recent_history)

    # Add current query
    messages.append({"role": "user", "content": user_prompt})

    # Call OpenAI API
    try:
        # Use more tokens for synthesis mode
        max_tokens = self.max_tokens + 200 if synthesis_mode else self.max_tokens

        response = await self.client.chat.completions.create(
            model=self.model,
            messages=messages,
            temperature=self.temperature,
            max_tokens=max_tokens,
            timeout=30.0
        )

        answer = response.choices[0].message.content
        tokens_used = response.usage.total_tokens if response.usage else 0

        # Build Citation objects with multi-source awareness
        citations = self._build_citations(chunks, answer, synthesis_mode)

        latency_ms = int((time.time() - start_time) * 1000)

        # Log diagnostics
        logger.info(
            "answer_generated",
            request_id=request_id,
            latency_ms=latency_ms,
            tokens_used=tokens_used,
            chunks_used=len(chunks),
            answer_length=len(answer),
            synthesis_mode=synthesis_mode,
            source_doc_count=source_doc_count
        )

        return {
            "answer": answer,
            "citations": citations,
            "latency_ms": latency_ms,
            "tokens_used": tokens_used,
            "synthesis_mode": synthesis_mode,
            "source_doc_count": source_doc_count
        }

    except Exception as e:
        logger.error(
            "generation_failed",
            request_id=request_id,
            error=str(e),
            error_type=type(e).__name__
        )
        raise Exception(f"Failed to generate answer: {str(e)}") from e
```

Add helper method for citation building with multi-source awareness:

```python
def _build_citations(
    self,
    chunks: List[Dict[str, Any]],
    answer: str,
    synthesis_mode: bool
) -> List[Citation]:
    """
    Build Citation objects with multi-source awareness.

    Detects citation ranges in answer (e.g., [1-3]) for compact notation.
    """
    import re

    # Parse citation numbers from answer
    cited_numbers = set()
    citation_pattern = r'\[(\d+(?:-\d+)?(?:,\s*\d+)*)\]'

    for match in re.finditer(citation_pattern, answer):
        citation_ref = match.group(1)

        # Handle ranges like [1-3]
        if '-' in citation_ref:
            parts = citation_ref.split('-')
            try:
                start = int(parts[0].strip())
                end = int(parts[1].split(',')[0].strip())
                cited_numbers.update(range(start, end + 1))
            except ValueError:
                pass

        # Handle comma-separated like [1,2,3]
        for num in citation_ref.replace('-', ',').split(','):
            try:
                cited_numbers.add(int(num.strip()))
            except ValueError:
                pass

    citations = []
    for idx, chunk in enumerate(chunks, 1):
        snippet = chunk.get('text', '')[:200]
        if len(chunk.get('text', '')) > 200:
            snippet += "..."

        source_type = chunk.get('source', 'document')

        # Detect if part of multi-source claim (adjacent citations)
        is_multi_source = (
            synthesis_mode and
            ((idx - 1) in cited_numbers or (idx + 1) in cited_numbers) and
            idx in cited_numbers
        )

        citation = Citation(
            id=idx,
            doc_id=chunk.get('doc_id', ''),
            filename=chunk.get('filename', ''),
            page_range=chunk.get('page_range', ''),
            section_title=chunk.get('section_title'),
            snippet=snippet,
            score=chunk.get('rerank_score', chunk.get('score')),
            source=source_type,
            multi_source=is_multi_source
        )
        citations.append(citation)

    return citations
```

Update the import at the top of the file to include re if not present.
  </action>
  <verify>python -c "from backend.app.services.generator import Generator, should_activate_synthesis_mode, SYNTHESIS_SYSTEM_PROMPT; print('Generator OK')"</verify>
  <done>Generator uses synthesis mode for 2+ document queries with topic-organized prompts and Chain-of-Thought, returns synthesis_mode and source_doc_count</done>
</task>

<task type="auto">
  <name>Task 2: Update chat router to pass synthesis metadata</name>
  <files>backend/app/routers/chat.py</files>
  <action>
Update the chat router to pass synthesis metadata from generator to ChatResponse.

Find the chat endpoint (POST /api/chat or similar) and update the response construction to include synthesis_mode and source_doc_count from the generator result:

```python
# After generator.generate() call, update response:
return ChatResponse(
    answer=result["answer"],
    citations=result["citations"],
    request_id=request_id,
    diagnostics=diagnostics if include_diagnostics else None,
    synthesis_mode=result.get("synthesis_mode", False),
    source_doc_count=result.get("source_doc_count", 1)
)
```

If the chat router doesn't exist, this task is informational - the generator now returns the correct data structure and chat.py models are already updated to include these fields.
  </action>
  <verify>python -c "from backend.app.models.chat import ChatResponse; r = ChatResponse(answer='test', citations=[], request_id='r', synthesis_mode=True, source_doc_count=3); print(r.synthesis_mode, r.source_doc_count)"</verify>
  <done>ChatResponse includes synthesis_mode and source_doc_count fields populated from generator</done>
</task>

</tasks>

<verification>
1. Synthesis mode function exists: `python -c "from backend.app.services.generator import should_activate_synthesis_mode; print(should_activate_synthesis_mode([]))"`
2. SYNTHESIS_SYSTEM_PROMPT defined: `python -c "from backend.app.services.generator import SYNTHESIS_SYSTEM_PROMPT; print(len(SYNTHESIS_SYSTEM_PROMPT) > 100)"`
3. Generator returns synthesis fields: `python -c "from backend.app.services.generator import Generator; print('OK')"`
</verification>

<success_criteria>
- should_activate_synthesis_mode() detects 2+ documents with 2+ chunks each
- SYNTHESIS_SYSTEM_PROMPT uses topic-organized structure with consensus language
- Generator.generate() returns synthesis_mode and source_doc_count
- Citation building detects adjacent citations for multi_source flag
- Context grouped by document in synthesis mode for better LLM understanding
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-source-synthesis/05-03-SUMMARY.md`
</output>
