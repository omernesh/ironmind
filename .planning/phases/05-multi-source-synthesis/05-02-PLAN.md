---
phase: 05-multi-source-synthesis
plan: 02
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - backend/app/services/graph/cross_reference.py
  - backend/app/services/pipeline.py
autonomous: true

must_haves:
  truths:
    - "System detects explicit document citations using regex patterns (doc codes, 'See Document X')"
    - "System detects shared entity relationships when documents share 2+ common entities"
    - "Document relationships are stored in FalkorDB during ingestion pipeline"
    - "Explicit citations have strength 1.0, shared entities have strength 0.5-0.9"
  artifacts:
    - path: "backend/app/services/graph/cross_reference.py"
      provides: "Cross-reference detection logic"
      exports: ["CrossReferenceDetector"]
      min_lines: 80
    - path: "backend/app/services/pipeline.py"
      provides: "Pipeline with document relationship extraction"
      contains: "cross_reference_detector"
  key_links:
    - from: "backend/app/services/graph/cross_reference.py"
      to: "backend/app/services/graph/graph_store.py"
      via: "queries entities by doc_id"
      pattern: "list_entities_for_doc"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/graph/doc_relationships.py"
      via: "stores detected relationships"
      pattern: "doc_rel_store\\.add_relationship"
---

<objective>
Implement cross-reference detection and integrate into document ingestion pipeline.

Purpose: Build the document relationship graph during ingestion by detecting explicit citations (doc codes, hyperlinks, "See Document X") and shared entity references (2+ common entities threshold).
Output: CrossReferenceDetector service, updated pipeline with document relationship extraction stage.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-multi-source-synthesis/05-CONTEXT.md
@.planning/phases/05-multi-source-synthesis/05-RESEARCH.md
@.planning/phases/05-multi-source-synthesis/05-01-SUMMARY.md
@backend/app/services/pipeline.py
@backend/app/services/graph/graph_store.py
@backend/app/services/retriever.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create CrossReferenceDetector service</name>
  <files>backend/app/services/graph/cross_reference.py</files>
  <action>
Create cross_reference.py with CrossReferenceDetector class that detects both explicit citations and shared entities:

```python
"""Cross-reference detection for document relationship graph.

Detects two types of document relationships:
1. Explicit citations: Document codes, "See Document X", hyperlinks
2. Shared entities: Documents with 2+ common entities (from knowledge graph)
"""
import re
from typing import List, Dict, Any, Set, Optional
import structlog

try:
    from Levenshtein import ratio as levenshtein_ratio
except ImportError:
    # Fallback if python-Levenshtein not installed
    def levenshtein_ratio(s1: str, s2: str) -> float:
        # Simple ratio based on length difference
        if not s1 or not s2:
            return 0.0
        return 1.0 - abs(len(s1) - len(s2)) / max(len(s1), len(s2))

from app.services.graph.schemas import DocumentRelationship
from app.services.graph.graph_store import GraphStore
from app.services.retriever import ACRONYM_MAP

logger = structlog.get_logger(__name__)


class CrossReferenceDetector:
    """Detects cross-references between documents using dual signals.

    Priority: Explicit citations (strength 1.0) > Shared entities (strength 0.5-0.9)
    Threshold: Requires 2+ shared entities to establish shared_entities relationship.
    """

    # Patterns for explicit citation detection
    DOC_CODE_PATTERN = re.compile(r'\b[A-Z]{2,}-[\d\.]+\b')  # FC-001, GC-v2.3
    SEE_DOC_PATTERN = re.compile(
        r'(?:see|refer to|described in|detailed in|according to)\s+([A-Za-z\s\-]+(?:manual|guide|specification|document|spec))',
        re.IGNORECASE
    )
    SECTION_REF_PATTERN = re.compile(
        r'section\s+[\d\.]+\s+of\s+([A-Za-z\s\-]+)',
        re.IGNORECASE
    )

    def __init__(self):
        self.graph_store = GraphStore()

    async def detect_cross_references(
        self,
        doc_id: str,
        doc_text: str,
        user_id: str,
        existing_docs: List[Dict[str, Any]]
    ) -> List[DocumentRelationship]:
        """
        Detect relationships between new document and existing documents.

        Args:
            doc_id: ID of document being ingested
            doc_text: Full text content of document
            user_id: User identifier for isolation
            existing_docs: List of existing documents [{doc_id, filename}]

        Returns:
            List of DocumentRelationship objects
        """
        relationships = []

        # Signal 1: Explicit citations (STRONGER - weight 1.0)
        explicit_refs = self._detect_explicit_references(doc_text, existing_docs)
        for ref in explicit_refs:
            relationships.append(DocumentRelationship(
                source_doc_id=doc_id,
                target_doc_id=ref["target_doc_id"],
                relationship_type="explicit_citation",
                strength=1.0,
                evidence=[ref["citation_text"]]
            ))

        # Signal 2: Shared entities (WEAKER - weight 0.5-0.9)
        shared_entity_refs = await self._detect_shared_entities(
            doc_id, user_id, existing_docs
        )
        for ref in shared_entity_refs:
            # Skip if already have explicit citation to this doc
            if any(r.target_doc_id == ref["target_doc_id"] and
                   r.relationship_type == "explicit_citation"
                   for r in relationships):
                continue

            relationships.append(DocumentRelationship(
                source_doc_id=doc_id,
                target_doc_id=ref["target_doc_id"],
                relationship_type="shared_entities",
                strength=ref["strength"],
                evidence=ref["shared_entities"]
            ))

        logger.info(
            "cross_references_detected",
            doc_id=doc_id,
            explicit_count=len(explicit_refs),
            shared_entity_count=len(shared_entity_refs),
            total_relationships=len(relationships)
        )

        return relationships

    def _detect_explicit_references(
        self,
        text: str,
        existing_docs: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Detect explicit document citations using pattern matching."""
        refs = []
        all_mentions = []

        # Pattern 1: Document codes (FC-001, GC-v2.3)
        codes = self.DOC_CODE_PATTERN.findall(text)
        all_mentions.extend(codes)

        # Pattern 2: "See [document name]" patterns
        see_refs = self.SEE_DOC_PATTERN.findall(text)
        all_mentions.extend(see_refs)

        # Pattern 3: Section references
        section_refs = self.SECTION_REF_PATTERN.findall(text)
        all_mentions.extend(section_refs)

        # Match against existing document filenames using fuzzy matching
        for mention in all_mentions:
            mention_clean = mention.strip().lower()
            for doc in existing_docs:
                filename = doc.get("filename", "").lower()
                # Remove extension for comparison
                filename_base = re.sub(r'\.[^.]+$', '', filename)

                # Fuzzy match (70% similarity threshold)
                similarity = levenshtein_ratio(mention_clean, filename_base)
                if similarity > 0.7:
                    refs.append({
                        "target_doc_id": doc["doc_id"],
                        "citation_text": mention,
                        "similarity": similarity
                    })
                    break

                # Also check if mention appears in filename
                if mention_clean in filename_base or filename_base in mention_clean:
                    refs.append({
                        "target_doc_id": doc["doc_id"],
                        "citation_text": mention,
                        "similarity": 0.8
                    })
                    break

        # Deduplicate by target_doc_id
        seen = set()
        unique_refs = []
        for ref in refs:
            if ref["target_doc_id"] not in seen:
                unique_refs.append(ref)
                seen.add(ref["target_doc_id"])

        return unique_refs

    async def _detect_shared_entities(
        self,
        doc_id: str,
        user_id: str,
        existing_docs: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Detect shared entity relationships (2+ common entities threshold)."""
        refs = []

        # Get entities for the new document
        source_entities = self.graph_store.list_entities_for_doc(doc_id, user_id)
        if not source_entities:
            return refs

        source_names = self._normalize_entity_names(source_entities)

        for target_doc in existing_docs:
            target_doc_id = target_doc["doc_id"]
            if target_doc_id == doc_id:
                continue

            # Get entities for target document
            target_entities = self.graph_store.list_entities_for_doc(target_doc_id, user_id)
            if not target_entities:
                continue

            target_names = self._normalize_entity_names(target_entities)

            # Find shared entities (exact match or alias match)
            shared = source_names & target_names
            shared_list = list(shared)

            # Require 2+ shared entities (per CONTEXT.md decision)
            if len(shared_list) >= 2:
                # Strength based on count: 0.5 + (count - 2) * 0.1, capped at 0.9
                strength = min(0.5 + (len(shared_list) - 2) * 0.1, 0.9)

                refs.append({
                    "target_doc_id": target_doc_id,
                    "shared_entities": shared_list[:10],  # Cap evidence list
                    "strength": strength
                })

        return refs

    def _normalize_entity_names(self, entities: List[Dict]) -> Set[str]:
        """Normalize entity names with acronym expansion for matching."""
        names = set()
        for entity in entities:
            name = entity.get("name", "").lower()
            names.add(name)

            # Add expanded acronym form
            name_upper = entity.get("name", "")
            if name_upper in ACRONYM_MAP:
                names.add(ACRONYM_MAP[name_upper].lower())

            # Also check if name IS an expansion, add acronym
            for acronym, expansion in ACRONYM_MAP.items():
                if expansion.lower() == name:
                    names.add(acronym.lower())

        return names
```

Add list_entities_for_doc method to GraphStore if not already present (in graph_store.py):
```python
def list_entities_for_doc(self, doc_id: str, user_id: str) -> List[Dict[str, Any]]:
    """List entities from a specific document."""
    try:
        query = """
        MATCH (e:Entity {doc_id: $doc_id, user_id: $user_id})
        RETURN e
        """
        params = {"doc_id": doc_id, "user_id": user_id}
        result = self.graph.query(query, params=params)

        return [dict(row[0].properties) for row in result.result_set]
    except Exception as e:
        logger.error("list_entities_for_doc_failed", doc_id=doc_id, error=str(e))
        return []
```

Update backend/app/services/graph/__init__.py to export CrossReferenceDetector.
  </action>
  <verify>python -c "from backend.app.services.graph.cross_reference import CrossReferenceDetector; print('Detector OK')"</verify>
  <done>CrossReferenceDetector class exists with detect_cross_references, _detect_explicit_references, _detect_shared_entities methods</done>
</task>

<task type="auto">
  <name>Task 2: Integrate document relationship extraction into pipeline</name>
  <files>backend/app/services/pipeline.py</files>
  <action>
Update DocumentPipeline to:
1. Import DocRelationshipStore and CrossReferenceDetector
2. Initialize stores in __init__
3. Add DOCUMENT_RELATIONSHIPS stage after GRAPH_EXTRACTING
4. Extract and store document relationships

Add imports at top:
```python
from app.services.graph.doc_relationships import DocumentRelationshipStore
from app.services.graph.cross_reference import CrossReferenceDetector
```

Update __init__ to initialize new components:
```python
def __init__(self):
    self.db = DocumentDatabase(settings.database_path)
    self.docling = DoclingClient()
    self.chunker = SemanticChunker()
    self.indexer = TxtaiIndexer()
    self.storage = StorageService(settings.DATA_DIR)
    self.extractor = EntityExtractor()
    self.graph_store = GraphStore()
    self.doc_rel_store = DocumentRelationshipStore()
    self.cross_ref_detector = CrossReferenceDetector()
```

Add new stage after GRAPH_EXTRACTING (before INDEXING). Insert this code block after the graph extraction stage and before Stage 4: INDEXING:

```python
# Stage 3.5: DOCUMENT RELATIONSHIPS
stage_start = time.time()
doc_relationship_count = 0

try:
    # Get list of existing documents for this user
    existing_docs = await self.db.list_user_documents(user_id)
    existing_doc_list = [
        {"doc_id": d.doc_id, "filename": d.filename}
        for d in existing_docs
        if d.doc_id != doc_id and d.status == ProcessingStatus.DONE
    ]

    if existing_doc_list:
        # Get full text for cross-reference detection
        doc_text = parse_result.get("md_content", "")
        if not doc_text:
            # Fallback to concatenated sections
            sections = parse_result.get("sections", [])
            doc_text = "\n".join(s.get("content", "") for s in sections)

        # Detect cross-references
        relationships = await self.cross_ref_detector.detect_cross_references(
            doc_id=doc_id,
            doc_text=doc_text,
            user_id=user_id,
            existing_docs=existing_doc_list
        )

        # Add document node
        self.doc_rel_store.add_document_node(
            doc_id=doc_id,
            filename=doc.filename,
            user_id=user_id,
            page_count=page_count,
            chunk_count=len(chunks)
        )

        # Store relationships
        for rel in relationships:
            if self.doc_rel_store.add_relationship(rel, user_id):
                doc_relationship_count += 1

    processing_log.append(ProcessingLogEntry(
        stage="DocumentRelationships",
        started_at=datetime.fromtimestamp(stage_start, timezone.utc),
        completed_at=datetime.now(timezone.utc),
        duration_ms=int((time.time() - stage_start) * 1000)
    ))

    logger.info("document_relationships_extracted",
               doc_id=doc_id,
               relationship_count=doc_relationship_count)

except Exception as e:
    # Log warning but continue (relationships are enhancement, not critical)
    logger.warning("document_relationship_extraction_failed",
                  doc_id=doc_id,
                  error=str(e))
    processing_log.append(ProcessingLogEntry(
        stage="DocumentRelationships",
        started_at=datetime.fromtimestamp(stage_start, timezone.utc),
        completed_at=datetime.now(timezone.utc),
        duration_ms=int((time.time() - stage_start) * 1000),
        error=str(e)
    ))
```

Also update STAGE_WEIGHTS to include DocumentRelationships:
```python
STAGE_WEIGHTS = {
    "Uploading": 0.10,
    "Parsing": 0.30,
    "Chunking": 0.15,
    "GraphExtracting": 0.15,
    "DocumentRelationships": 0.05,  # NEW
    "Indexing": 0.25,
    "Complete": 1.0
}
```

Add doc_relationship_count to the final document update:
```python
doc.doc_relationship_count = doc_relationship_count
```

NOTE: Also need to add list_user_documents method to DocumentDatabase if not present.
  </action>
  <verify>python -c "from backend.app.services.pipeline import DocumentPipeline; p = DocumentPipeline(); print('Pipeline OK')"</verify>
  <done>Pipeline includes document relationship extraction stage after graph extraction, stores relationships via DocRelationshipStore</done>
</task>

</tasks>

<verification>
1. CrossReferenceDetector imports: `python -c "from backend.app.services.graph import CrossReferenceDetector"`
2. Pipeline initializes new components: `python -c "from backend.app.services.pipeline import DocumentPipeline; p = DocumentPipeline(); print(hasattr(p, 'doc_rel_store'), hasattr(p, 'cross_ref_detector'))"`
3. STAGE_WEIGHTS includes DocumentRelationships: `python -c "from backend.app.services.pipeline import STAGE_WEIGHTS; print('DocumentRelationships' in STAGE_WEIGHTS)"`
</verification>

<success_criteria>
- CrossReferenceDetector detects explicit citations using regex patterns
- CrossReferenceDetector detects shared entities with 2+ threshold
- DocumentPipeline extracts and stores document relationships during ingestion
- Explicit citations have strength 1.0, shared entities have strength 0.5-0.9
- Graceful error handling (relationship extraction failure doesn't crash pipeline)
</success_criteria>

<output>
After completion, create `.planning/phases/05-multi-source-synthesis/05-02-SUMMARY.md`
</output>
