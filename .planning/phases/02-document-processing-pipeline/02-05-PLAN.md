---
phase: 02-document-processing-pipeline
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/services/chunker.py
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "Chunker extracts sections from docling md_content markdown format"
    - "Documents are parsed into 1+ chunks with metadata"
    - "Chunks are indexed in txtai with embeddings"
  artifacts:
    - path: "backend/app/services/chunker.py"
      provides: "Markdown section extraction from docling output"
      contains: "md_content"
  key_links:
    - from: "pipeline.py"
      to: "chunker.py"
      via: "chunk_document(docling_output)"
      pattern: "chunk_document"
---

<objective>
Fix docling output format mismatch so semantic chunking produces actual chunks.

Purpose: Close verification gaps - docling returns `{"document": {"md_content": "..."}}` but chunker expects `{"sections": [...]}`. Pipeline runs but produces 0 chunks for all documents.

Output: Chunker that parses markdown from docling output, extracts sections via markdown headings, and produces indexed chunks.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing-pipeline/02-VERIFICATION.md

# Existing chunker implementation
@backend/app/services/chunker.py

# Pipeline that calls chunker
@backend/app/services/pipeline.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix _extract_sections to parse docling md_content format</name>
  <files>backend/app/services/chunker.py</files>
  <action>
Update SemanticChunker._extract_sections() to handle docling v1.10.0 output format.

**Docling v1.10.0 actual output structure:**
```json
{
  "document": {
    "md_content": "# Heading 1\n\nParagraph text...\n\n## Heading 2\n\nMore text...",
    "json_content": null,
    "text_content": ""
  }
}
```

**Required changes:**

1. Add new extraction path in _extract_sections() to detect and handle document.md_content:
```python
# Check for docling v1.10.0 format (document.md_content)
if "document" in docling_output:
    doc = docling_output["document"]
    md_content = doc.get("md_content", "")
    if md_content:
        return self._parse_markdown_sections(md_content)
```

2. Add new method _parse_markdown_sections() that:
   - Splits markdown by heading patterns (^#{1,6}\s+.+$)
   - Creates sections from each heading block
   - Extracts heading text as section title
   - Extracts content until next heading as section text
   - Estimates page numbers (1 page per ~3000 chars as rough estimate)

```python
def _parse_markdown_sections(self, md_content: str) -> List[Dict]:
    """Parse markdown content into sections by headings."""
    import re

    sections = []
    # Match markdown headings (# Heading, ## Subheading, etc.)
    heading_pattern = re.compile(r'^(#{1,6})\s+(.+)$', re.MULTILINE)

    # Find all headings and their positions
    headings = list(heading_pattern.finditer(md_content))

    if not headings:
        # No headings found - treat entire content as one section
        return [{"title": "Document", "text": md_content.strip(), "page_range": "1"}]

    for i, match in enumerate(headings):
        title = match.group(2).strip()
        start = match.end()

        # End is either next heading or end of document
        if i + 1 < len(headings):
            end = headings[i + 1].start()
        else:
            end = len(md_content)

        text = md_content[start:end].strip()

        # Estimate page number (rough: 3000 chars per page)
        page_estimate = max(1, (match.start() // 3000) + 1)

        if text:  # Only add sections with content
            sections.append({
                "title": title,
                "text": text,
                "page_range": str(page_estimate)
            })

    return sections
```

3. Update fallback in chunk_document() to also check document.md_content:
```python
if not sections:
    logger.warning("no_sections_found", doc_id=doc_id)
    # Fallback: check for document.md_content or top-level text
    full_text = ""
    if "document" in docling_output:
        full_text = docling_output["document"].get("md_content", "")
    if not full_text:
        full_text = docling_output.get("text", "")
    if full_text:
        sections = [{"title": "Document", "text": full_text, "page_range": "1"}]
```

**Important:** Keep existing extraction paths for sections/pages format as fallback for other docling configurations.
  </action>
  <verify>
Run Python import check to verify syntax:
```bash
cd backend && python -c "from app.services.chunker import SemanticChunker; print('OK')"
```

Test extraction logic with mock docling output:
```bash
cd backend && python -c "
from app.services.chunker import SemanticChunker
chunker = SemanticChunker()

# Mock docling v1.10.0 output
mock_output = {
    'document': {
        'md_content': '''# Introduction
This is intro text.

## Background
Background content here with more detail.

### Technical Details
Technical specifications and details.

## Conclusion
Final thoughts.
'''
    }
}

sections = chunker._extract_sections(mock_output)
print(f'Sections extracted: {len(sections)}')
for s in sections:
    print(f'  - {s[\"title\"]}: {len(s[\"text\"])} chars')
assert len(sections) >= 4, 'Expected at least 4 sections'
print('PASS')
"
```
  </verify>
  <done>
_extract_sections() returns 4+ sections from markdown headings. Mock test passes with sections having titles and content.
  </done>
</task>

<task type="auto">
  <name>Task 2: Re-process existing documents and verify chunks created</name>
  <files>None (verification only)</files>
  <action>
Re-trigger document processing for an existing document to verify the fix works end-to-end.

1. First check if there are existing documents in "Done" status with 0 chunks:
```bash
docker exec ironmind-backend python -c "
import asyncio
from app.core.database import DocumentDatabase

async def check():
    db = DocumentDatabase()
    await db.initialize()
    docs = await db.list_documents_by_user('test-user-id')
    for d in docs:
        print(f'{d.doc_id}: status={d.status}, chunks={d.chunk_count}')

asyncio.run(check())
"
```

2. If documents exist, delete and re-upload a test document:
```bash
# Upload a small test PDF
curl -X POST http://localhost:8000/api/documents/upload \
  -H "Authorization: Bearer $TEST_TOKEN" \
  -F "files=@/path/to/test.pdf"
```

3. Poll status until processing completes:
```bash
# Replace {doc_id} with actual doc_id from upload response
curl http://localhost:8000/api/documents/{doc_id}/status \
  -H "Authorization: Bearer $TEST_TOKEN"
```

4. Verify chunks were created:
```bash
docker exec ironmind-backend python -c "
import asyncio
from app.core.database import DocumentDatabase

async def check():
    db = DocumentDatabase()
    await db.initialize()
    doc = await db.get_document('{doc_id}')  # Replace with actual doc_id
    print(f'Status: {doc.status}')
    print(f'Chunk count: {doc.chunk_count}')
    assert doc.chunk_count > 0, 'Expected chunks to be created'
    print('PASS')

asyncio.run(check())
"
```

**Note:** If no documents exist or backend is not running, verification can be deferred to manual testing. The primary verification is Task 1's unit test of the extraction logic.
  </action>
  <verify>
If backend is running and documents exist:
- Document status shows "Done" or "Indexed"
- chunk_count > 0

If backend not running:
- Task 1 verification passes (unit test of extraction logic)
- Mark as "deferred to integration testing"
  </verify>
  <done>
Either: Documents re-processed with chunk_count > 0
Or: Task 1 unit tests pass and integration testing deferred
  </done>
</task>

</tasks>

<verification>
**Gap closure verification:**

1. **Gap 1 closed:** _extract_sections() handles document.md_content format
   - Evidence: Unit test extracts 4+ sections from mock markdown
   - Evidence: No more "no_sections_found" log entries for new uploads

2. **Gap 2 closed:** Semantic chunking produces chunks
   - Evidence: chunk_count > 0 after document processing
   - Evidence: txtai index directory contains embeddings
</verification>

<success_criteria>
1. SemanticChunker._extract_sections() parses docling document.md_content format
2. Mock markdown test extracts sections with titles and content
3. New document uploads produce chunk_count > 0
4. Phase 2 verification can be re-run and pass 6/6 truths
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing-pipeline/02-05-SUMMARY.md`
</output>
