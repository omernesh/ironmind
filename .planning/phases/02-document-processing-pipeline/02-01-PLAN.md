---
phase: 02-document-processing-pipeline
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - backend/app/models/documents.py
  - backend/app/services/storage.py
  - backend/app/core/database.py
  - backend/app/config.py
  - backend/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Document model can store doc_id, user_id, filename, status, timestamps"
    - "File storage creates /data/raw/{user_id}/{doc_id}/ directories"
    - "File storage creates /data/processed/{user_id}/{doc_id}/ directories"
    - "Document status can transition through Uploading, Parsing, Chunking, Indexing, Done, Failed"
    - "Processing log tracks timestamps and durations per stage"
  artifacts:
    - path: "backend/app/models/documents.py"
      provides: "Document model, ProcessingStatus enum, ChunkMetadata model"
      min_lines: 80
    - path: "backend/app/services/storage.py"
      provides: "File storage with path validation and directory creation"
      min_lines: 60
    - path: "backend/app/core/database.py"
      provides: "SQLite database manager for document tracking"
      min_lines: 50
  key_links:
    - from: "backend/app/services/storage.py"
      to: "pathlib.Path"
      via: "secure path validation"
      pattern: "resolve\\(\\)|is_relative_to"
---

<objective>
Create document storage layer with models, status tracking, and secure file storage patterns.

Purpose: Foundation for document upload and processing - defines data structures, storage paths, and status tracking that all subsequent plans build upon.

Output: Document models, storage service, database schema for tracking document processing state.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing-pipeline/02-RESEARCH.md
@.planning/phases/02-document-processing-pipeline/02-CONTEXT.md
@backend/app/config.py
@backend/app/main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create document models and database schema</name>
  <files>
    backend/app/models/__init__.py
    backend/app/models/documents.py
    backend/app/core/database.py
    backend/app/config.py
    backend/requirements.txt
  </files>
  <action>
Create document data models and SQLite database layer for document tracking:

1. **backend/app/models/documents.py:**
   - `ProcessingStatus` enum: UPLOADING, PARSING, CHUNKING, INDEXING, DONE, FAILED
   - `ProcessingLogEntry` model: stage (str), started_at (datetime), completed_at (datetime), duration_ms (int), error (Optional[str])
   - `Document` model with fields:
     - doc_id (str, UUID format)
     - user_id (str)
     - filename (str)
     - file_type (str, "pdf" or "docx")
     - file_size_bytes (int)
     - status (ProcessingStatus)
     - current_stage (str)
     - error (Optional[str])
     - processing_log (List[ProcessingLogEntry])
     - page_count (Optional[int])
     - chunk_count (Optional[int])
     - created_at (datetime)
     - updated_at (datetime)
   - `ChunkMetadata` model: chunk_id (str format doc_id-chunk-NNN), doc_id, user_id, filename, section_title, page_range, chunk_index, token_count, created_at
   - Use Pydantic BaseModel for all models

2. **backend/app/core/database.py:**
   - `DocumentDatabase` class using SQLite (similar pattern to frontend auth.db)
   - Create `documents` table with schema matching Document model
   - Store processing_log as JSON string
   - Methods: create_document, get_document, update_document, list_documents_by_user, delete_document
   - Use aiosqlite for async operations
   - WAL mode for concurrency
   - Initialize database path from settings

3. **backend/app/config.py:**
   - Add DATA_DIR setting (default: "/app/data")
   - Add DOCLING_URL setting (default: "http://docling:5000")
   - Add MAX_FILE_SIZE_MB setting (default: 10)
   - Add MAX_DOCUMENTS_PER_USER setting (default: 10)
   - Add DATABASE_PATH setting (default: "{DATA_DIR}/documents.db")

4. **backend/requirements.txt:**
   - Add: aiosqlite, aiofiles, tiktoken, backoff, httpx

5. **backend/app/models/__init__.py:**
   - Export Document, ProcessingStatus, ChunkMetadata, ProcessingLogEntry
  </action>
  <verify>
  - `python -c "from app.models.documents import Document, ProcessingStatus, ChunkMetadata; print('Models OK')"` succeeds
  - `python -c "from app.core.database import DocumentDatabase; print('DB OK')"` succeeds
  - `python -c "from app.config import settings; print(f'DATA_DIR={settings.DATA_DIR}')"` shows configured path
  </verify>
  <done>
  - Document, ProcessingStatus, ChunkMetadata, ProcessingLogEntry models importable
  - DocumentDatabase class with CRUD methods defined
  - Config settings include DATA_DIR, DOCLING_URL, MAX_FILE_SIZE_MB, MAX_DOCUMENTS_PER_USER
  - Dependencies added to requirements.txt
  </done>
</task>

<task type="auto">
  <name>Task 2: Create secure file storage service</name>
  <files>
    backend/app/services/__init__.py
    backend/app/services/storage.py
  </files>
  <action>
Create file storage service with secure path handling to prevent traversal attacks:

1. **backend/app/services/storage.py:**
   - `StorageService` class initialized with base_path from settings.DATA_DIR
   - `sanitize_filename(filename: str) -> str`: Remove path components, keep only alphanumeric + dots + hyphens + underscores, limit to 255 chars. Use regex: `re.sub(r'[^a-zA-Z0-9._-]', '_', filename)`
   - `get_raw_path(user_id: str, doc_id: str, filename: str) -> Path`: Returns validated path at {base}/raw/{user_id}/{doc_id}/{sanitized_filename}
   - `get_processed_path(user_id: str, doc_id: str) -> Path`: Returns validated path at {base}/processed/{user_id}/{doc_id}/
   - `validate_path(path: Path, base: Path) -> bool`: Use path.resolve() and check is_relative_to(base) to prevent traversal
   - `save_upload(user_id: str, doc_id: str, filename: str, content: bytes) -> Path`: Creates directories, validates path, saves file, returns path
   - `save_processed_json(user_id: str, doc_id: str, data: dict) -> Path`: Save docling output as JSON
   - `delete_document_files(user_id: str, doc_id: str)`: Delete both raw and processed directories
   - Use aiofiles for async file operations
   - Always validate paths before any file operation
   - Log all file operations with structlog

2. **backend/app/services/__init__.py:**
   - Export StorageService

**Security requirements (from RESEARCH.md pitfall #2):**
- NEVER use user-provided filename directly in path
- ALWAYS sanitize filename before path construction
- ALWAYS resolve() paths and verify is_relative_to base directory
- Log and raise ValueError on path traversal attempt
  </action>
  <verify>
  - `python -c "from app.services.storage import StorageService; s = StorageService('/tmp/test'); print(s.sanitize_filename('../../../etc/passwd'))"` returns safe filename without path components
  - `python -c "from app.services.storage import StorageService; s = StorageService('/tmp/test'); print(s.get_raw_path('user1', 'doc1', 'test.pdf'))"` returns valid path structure
  </verify>
  <done>
  - StorageService with secure path validation implemented
  - sanitize_filename removes all path traversal attempts
  - Path validation uses resolve() and is_relative_to()
  - Async file save and delete operations work
  - All operations logged with structlog
  </done>
</task>

</tasks>

<verification>
Run from backend directory:
```bash
cd backend
pip install -r requirements.txt
python -c "
from app.models.documents import Document, ProcessingStatus, ChunkMetadata, ProcessingLogEntry
from app.core.database import DocumentDatabase
from app.services.storage import StorageService
from app.config import settings

# Verify models
assert ProcessingStatus.UPLOADING.value == 'Uploading'
assert ProcessingStatus.DONE.value == 'Done'

# Verify config
assert settings.DATA_DIR
assert settings.DOCLING_URL
assert settings.MAX_FILE_SIZE_MB == 10
assert settings.MAX_DOCUMENTS_PER_USER == 10

# Verify storage security
s = StorageService(settings.DATA_DIR)
bad_name = s.sanitize_filename('../../../etc/passwd')
assert '..' not in bad_name
assert '/' not in bad_name

print('All verifications passed')
"
```
</verification>

<success_criteria>
- Document and chunk models define complete metadata schema
- DocumentDatabase provides async CRUD operations with SQLite
- StorageService prevents path traversal attacks
- Config settings match Phase 2 requirements
- All new dependencies installable
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing-pipeline/02-01-SUMMARY.md`
</output>
