---
phase: 02-document-processing-pipeline
plan: 03
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/app/services/chunker.py
  - backend/app/services/indexer.py
autonomous: true

must_haves:
  truths:
    - "Chunker splits documents at section boundaries (never mid-section)"
    - "Chunks target ~1000 tokens with 10-20% overlap"
    - "Short sections (<50 tokens) merge with next section"
    - "Each chunk has metadata: doc_id, page_range, section_title, chunk_index, user_id"
    - "Deduplication removes identical chunks (by hash)"
    - "Indexer stores chunks in txtai with content storage enabled"
  artifacts:
    - path: "backend/app/services/chunker.py"
      provides: "Semantic chunking with section boundaries and overlap"
      min_lines: 120
    - path: "backend/app/services/indexer.py"
      provides: "txtai indexing with metadata"
      min_lines: 80
  key_links:
    - from: "backend/app/services/chunker.py"
      to: "tiktoken"
      via: "token counting for chunk sizing"
      pattern: "tiktoken\\.get_encoding|encoder\\.encode"
    - from: "backend/app/services/indexer.py"
      to: "txtai"
      via: "embeddings with content storage"
      pattern: "Embeddings|content.*True"
---

<objective>
Create semantic chunking pipeline with section-aware splitting and txtai indexing.

Purpose: Transform docling output into searchable chunks that preserve document structure and enable accurate RAG retrieval.

Output: Chunker service for section-boundary splitting, indexer service for txtai storage.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing-pipeline/02-RESEARCH.md
@.planning/phases/02-document-processing-pipeline/02-CONTEXT.md
@.planning/phases/02-document-processing-pipeline/02-01-SUMMARY.md
@backend/app/models/documents.py
@backend/app/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create semantic chunking service</name>
  <files>
    backend/app/services/chunker.py
  </files>
  <action>
Create semantic chunker that respects section boundaries and adds overlap:

**backend/app/services/chunker.py:**

```python
import hashlib
import tiktoken
from typing import List, Dict, Optional, Any
from datetime import datetime, timezone
from app.core.logging import get_logger
from app.models.documents import ChunkMetadata

logger = get_logger()

class SemanticChunker:
    """
    Chunk documents by section boundaries with overlap.

    Strategy:
    1. Respect section boundaries (never split mid-section)
    2. Merge sections < 50 tokens with next section
    3. Split large sections at ~1000 token boundaries
    4. Add 10-20% overlap between consecutive chunks
    """

    def __init__(
        self,
        target_tokens: int = 1000,
        overlap_pct: float = 0.15,
        min_section_tokens: int = 50
    ):
        self.target_tokens = target_tokens
        self.overlap_pct = overlap_pct
        self.min_section_tokens = min_section_tokens
        # Use cl100k_base encoding (GPT-3.5/4 compatible)
        self.encoder = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.encoder.encode(text))

    def chunk_document(
        self,
        docling_output: Dict[str, Any],
        doc_id: str,
        user_id: str,
        filename: str
    ) -> List[ChunkMetadata]:
        """
        Chunk docling output into semantic chunks with metadata.

        Args:
            docling_output: Parsed document from docling-serve
            doc_id: Document ID
            user_id: User ID
            filename: Original filename

        Returns:
            List of ChunkMetadata with text and metadata
        """
        sections = self._extract_sections(docling_output)

        if not sections:
            logger.warning("no_sections_found", doc_id=doc_id)
            # Fallback: treat entire text as one section
            full_text = docling_output.get("text", "")
            if full_text:
                sections = [{"title": "Document", "text": full_text, "page_range": "1"}]

        # Merge small sections
        merged_sections = self._merge_small_sections(sections)

        # Split large sections
        split_sections = self._split_large_sections(merged_sections)

        # Create chunks with metadata
        chunks = self._create_chunks(split_sections, doc_id, user_id, filename)

        # Add overlap
        chunks_with_overlap = self._add_overlap(chunks)

        # Deduplicate
        unique_chunks = self._deduplicate(chunks_with_overlap)

        logger.info("chunking_completed",
                   doc_id=doc_id,
                   sections=len(sections),
                   chunks=len(unique_chunks))

        return unique_chunks

    def _extract_sections(self, docling_output: Dict) -> List[Dict]:
        """Extract sections from docling output format."""
        sections = []

        # Docling may return different structures
        # Try common formats
        if "sections" in docling_output:
            for section in docling_output["sections"]:
                sections.append({
                    "title": section.get("heading", section.get("title", "")),
                    "text": section.get("text", section.get("content", "")),
                    "page_range": section.get("page_range",
                                             str(section.get("page", "")))
                })
        elif "pages" in docling_output:
            # Page-based structure
            for page in docling_output["pages"]:
                page_num = page.get("page_number", "")
                for block in page.get("blocks", []):
                    sections.append({
                        "title": block.get("heading", ""),
                        "text": block.get("text", ""),
                        "page_range": str(page_num)
                    })

        return sections

    def _merge_small_sections(self, sections: List[Dict]) -> List[Dict]:
        """Merge sections smaller than min_section_tokens."""
        if not sections:
            return []

        merged = []
        current = None

        for section in sections:
            token_count = self.count_tokens(section["text"])

            if current is None:
                current = section.copy()
            elif token_count < self.min_section_tokens:
                # Merge with current
                current["text"] += "\n\n" + section["text"]
                if section["page_range"]:
                    current["page_range"] = self._merge_page_ranges(
                        current["page_range"], section["page_range"]
                    )
            else:
                # Current section is complete
                merged.append(current)
                current = section.copy()

        if current:
            merged.append(current)

        return merged

    def _split_large_sections(self, sections: List[Dict]) -> List[Dict]:
        """Split sections larger than 1.5x target into multiple chunks."""
        result = []
        max_tokens = int(self.target_tokens * 1.5)

        for section in sections:
            token_count = self.count_tokens(section["text"])

            if token_count <= max_tokens:
                result.append(section)
            else:
                # Split at paragraph boundaries
                paragraphs = section["text"].split("\n\n")
                current_text = ""
                current_tokens = 0
                chunk_idx = 0

                for para in paragraphs:
                    para_tokens = self.count_tokens(para)

                    if current_tokens + para_tokens > self.target_tokens and current_text:
                        result.append({
                            "title": f"{section['title']} (part {chunk_idx + 1})",
                            "text": current_text.strip(),
                            "page_range": section["page_range"]
                        })
                        current_text = para + "\n\n"
                        current_tokens = para_tokens
                        chunk_idx += 1
                    else:
                        current_text += para + "\n\n"
                        current_tokens += para_tokens

                if current_text.strip():
                    result.append({
                        "title": f"{section['title']} (part {chunk_idx + 1})" if chunk_idx > 0 else section["title"],
                        "text": current_text.strip(),
                        "page_range": section["page_range"]
                    })

        return result

    def _create_chunks(
        self,
        sections: List[Dict],
        doc_id: str,
        user_id: str,
        filename: str
    ) -> List[ChunkMetadata]:
        """Create ChunkMetadata objects from sections."""
        chunks = []

        for idx, section in enumerate(sections):
            chunk_id = f"{doc_id}-chunk-{idx:03d}"
            token_count = self.count_tokens(section["text"])

            chunks.append(ChunkMetadata(
                chunk_id=chunk_id,
                doc_id=doc_id,
                user_id=user_id,
                filename=filename,
                section_title=section["title"],
                page_range=section["page_range"],
                chunk_index=idx,
                token_count=token_count,
                text=section["text"],
                created_at=datetime.now(timezone.utc)
            ))

        return chunks

    def _add_overlap(self, chunks: List[ChunkMetadata]) -> List[ChunkMetadata]:
        """Add overlap from previous chunk to current chunk."""
        if len(chunks) <= 1:
            return chunks

        overlap_tokens = int(self.target_tokens * self.overlap_pct)
        result = [chunks[0]]

        for i in range(1, len(chunks)):
            prev_text = chunks[i-1].text
            prev_tokens = self.encoder.encode(prev_text)

            # Take last N tokens from previous chunk
            overlap_text = self.encoder.decode(prev_tokens[-overlap_tokens:]) if len(prev_tokens) > overlap_tokens else ""

            # Prepend to current chunk
            chunk = chunks[i]
            chunk.text = overlap_text + " " + chunk.text
            chunk.token_count = self.count_tokens(chunk.text)
            result.append(chunk)

        return result

    def _deduplicate(self, chunks: List[ChunkMetadata]) -> List[ChunkMetadata]:
        """Remove duplicate chunks by content hash."""
        seen_hashes = set()
        unique = []

        for chunk in chunks:
            # Normalize: lowercase, strip whitespace
            normalized = chunk.text.lower().strip()
            content_hash = hashlib.sha256(normalized.encode()).hexdigest()

            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique.append(chunk)

        if len(chunks) != len(unique):
            logger.info("chunks_deduplicated",
                       original=len(chunks),
                       unique=len(unique))

        return unique

    def _merge_page_ranges(self, range1: str, range2: str) -> str:
        """Merge two page ranges into a combined range."""
        try:
            # Parse ranges like "1-3" or "5"
            pages = set()
            for r in [range1, range2]:
                if "-" in r:
                    start, end = r.split("-")
                    pages.update(range(int(start), int(end) + 1))
                elif r.isdigit():
                    pages.add(int(r))

            if pages:
                return f"{min(pages)}-{max(pages)}"
        except:
            pass
        return range1 or range2
```

Key implementation details:
- tiktoken cl100k_base encoding (GPT-3.5/4 compatible)
- Section boundary respect (never split mid-section)
- Small section merging (<50 tokens)
- Large section splitting (>1500 tokens) at paragraph boundaries
- 15% overlap between consecutive chunks
- SHA-256 hash-based deduplication
  </action>
  <verify>
  - `python -c "from app.services.chunker import SemanticChunker; c = SemanticChunker(); print(c.count_tokens('Hello world'))"` returns token count
  - `python -c "from app.services.chunker import SemanticChunker; print('Chunker OK')"` imports successfully
  </verify>
  <done>
  - SemanticChunker with section-aware splitting
  - Small section merging (<50 tokens)
  - Large section splitting at paragraph boundaries
  - 15% overlap between chunks
  - SHA-256 deduplication
  - ChunkMetadata with all required fields
  </done>
</task>

<task type="auto">
  <name>Task 2: Create txtai indexer service</name>
  <files>
    backend/app/services/indexer.py
    backend/requirements.txt
  </files>
  <action>
Create txtai indexer that stores chunks with metadata for retrieval:

**backend/app/services/indexer.py:**

```python
from pathlib import Path
from typing import List, Optional, Dict, Any
from txtai.embeddings import Embeddings
from app.core.logging import get_logger
from app.models.documents import ChunkMetadata
from app.config import settings

logger = get_logger()

class TxtaiIndexer:
    """
    Txtai-based indexer for document chunks.

    Uses content storage to persist full text and metadata
    alongside vector embeddings for retrieval.
    """

    def __init__(self, index_path: Optional[str] = None):
        self.index_path = Path(index_path or f"{settings.DATA_DIR}/index")
        self.index_path.mkdir(parents=True, exist_ok=True)

        self.embeddings = None
        self._initialize_embeddings()

    def _initialize_embeddings(self):
        """Initialize txtai embeddings with content storage."""
        config = {
            # Use OpenAI embeddings (configured via env var OPENAI_API_KEY)
            "path": "sentence-transformers/all-MiniLM-L6-v2",  # Fallback for POC
            "content": True,  # Enable metadata storage - CRITICAL
            "backend": "sqlite",
            "functions": [
                {"name": "user_filter", "function": "app.services.indexer.user_filter"}
            ]
        }

        self.embeddings = Embeddings(config)

        # Load existing index if present
        if (self.index_path / "embeddings").exists():
            self.embeddings.load(str(self.index_path))
            logger.info("index_loaded", path=str(self.index_path))

    def index_chunks(self, chunks: List[ChunkMetadata], user_id: str, doc_id: str) -> int:
        """
        Index chunks for a document.

        Args:
            chunks: List of ChunkMetadata to index
            user_id: User ID for filtering
            doc_id: Document ID

        Returns:
            Number of chunks indexed
        """
        if not chunks:
            logger.warning("no_chunks_to_index", doc_id=doc_id)
            return 0

        # Convert to txtai format: (id, document_dict, tags)
        documents = []
        for chunk in chunks:
            doc = {
                "id": chunk.chunk_id,
                "text": chunk.text,
                "doc_id": chunk.doc_id,
                "user_id": chunk.user_id,
                "filename": chunk.filename,
                "section_title": chunk.section_title,
                "page_range": chunk.page_range,
                "chunk_index": chunk.chunk_index,
                "token_count": chunk.token_count,
                "created_at": chunk.created_at.isoformat()
            }
            documents.append((chunk.chunk_id, doc, None))

        # Index documents
        self.embeddings.index(documents)

        # Save index
        self.embeddings.save(str(self.index_path))

        logger.info("chunks_indexed",
                   doc_id=doc_id,
                   user_id=user_id,
                   count=len(chunks))

        return len(chunks)

    def delete_document_chunks(self, doc_id: str) -> int:
        """Delete all chunks for a document."""
        # Query for all chunks with this doc_id
        try:
            results = self.embeddings.search(
                f"SELECT id FROM txtai WHERE doc_id = '{doc_id}'",
                limit=10000
            )

            if results:
                chunk_ids = [r["id"] for r in results]
                self.embeddings.delete(chunk_ids)
                self.embeddings.save(str(self.index_path))
                logger.info("chunks_deleted", doc_id=doc_id, count=len(chunk_ids))
                return len(chunk_ids)
        except Exception as e:
            logger.warning("delete_chunks_failed", doc_id=doc_id, error=str(e))

        return 0

    def search(
        self,
        query: str,
        user_id: str,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        Search for chunks matching query, filtered by user_id.

        Args:
            query: Search query
            user_id: User ID to filter results
            limit: Maximum results

        Returns:
            List of matching chunks with scores
        """
        # Use SQL for metadata filtering
        sql = f"""
            SELECT id, text, doc_id, filename, section_title, page_range, score
            FROM txtai
            WHERE user_id = '{user_id}'
            ORDER BY score DESC
            LIMIT {limit}
        """

        results = self.embeddings.search(query, limit=limit)

        # Filter by user_id (txtai SQL filtering)
        filtered = [
            r for r in results
            if r.get("user_id") == user_id
        ]

        return filtered[:limit]

    def get_document_chunks(self, doc_id: str, user_id: str) -> List[Dict]:
        """Get all chunks for a document."""
        try:
            results = self.embeddings.search(
                f"SELECT * FROM txtai WHERE doc_id = '{doc_id}' AND user_id = '{user_id}' ORDER BY chunk_index",
                limit=10000
            )
            return results
        except Exception as e:
            logger.warning("get_chunks_failed", doc_id=doc_id, error=str(e))
            return []


def user_filter(user_id: str, doc_user_id: str) -> bool:
    """Filter function for user_id matching."""
    return user_id == doc_user_id
```

**backend/requirements.txt (add):**
```
txtai[pipeline]>=7.0.0
```

Key implementation details:
- content: True enables metadata storage (CRITICAL for filtering)
- SQLite backend for persistence
- User-based filtering for multi-tenant isolation
- Index saved after each operation
- Fallback to local embeddings for POC (OpenAI in Phase 3)
  </action>
  <verify>
  - `pip install "txtai[pipeline]>=7.0.0"` installs successfully
  - `python -c "from app.services.indexer import TxtaiIndexer; print('Indexer OK')"` imports successfully
  - `python -c "from txtai.embeddings import Embeddings; e = Embeddings({'content': True}); print('txtai OK')"` works
  </verify>
  <done>
  - TxtaiIndexer with content storage enabled
  - index_chunks stores chunks with full metadata
  - delete_document_chunks removes document's chunks
  - search filters by user_id for isolation
  - Index persisted to disk after operations
  </done>
</task>

</tasks>

<verification>
Test chunking and indexing together:
```python
# Test script
from app.services.chunker import SemanticChunker
from app.services.indexer import TxtaiIndexer
from datetime import datetime, timezone

# Mock docling output
docling_output = {
    "sections": [
        {"heading": "Introduction", "text": "This document describes the aircraft control systems. " * 50, "page_range": "1-2"},
        {"heading": "Control Surfaces", "text": "The primary control surfaces include ailerons, elevators, and rudder. " * 100, "page_range": "3-5"},
        {"heading": "Summary", "text": "Brief summary.", "page_range": "6"}
    ]
}

# Chunk document
chunker = SemanticChunker()
chunks = chunker.chunk_document(docling_output, "doc123", "user456", "manual.pdf")
print(f"Created {len(chunks)} chunks")

# Index chunks
indexer = TxtaiIndexer("/tmp/test_index")
indexed = indexer.index_chunks(chunks, "user456", "doc123")
print(f"Indexed {indexed} chunks")

# Search
results = indexer.search("control surfaces", "user456", limit=3)
print(f"Found {len(results)} results")
for r in results:
    print(f"  - {r.get('section_title', 'N/A')}: {r.get('text', '')[:50]}...")
```
</verification>

<success_criteria>
- Chunker respects section boundaries
- Small sections merged, large sections split
- 15% overlap between chunks
- Deduplication removes exact duplicates
- txtai indexes with content storage
- Search filters by user_id
- All chunk metadata preserved
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing-pipeline/02-03-SUMMARY.md`
</output>
