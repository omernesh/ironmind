---
phase: 02-document-processing-pipeline
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - backend/app/services/docling_client.py
  - backend/app/routers/documents.py
  - backend/app/main.py
autonomous: true

must_haves:
  truths:
    - "POST /api/documents/upload accepts multipart file upload"
    - "Upload validates file type (PDF, DOCX only) and size (max 10MB)"
    - "Upload enforces 10 document limit per user"
    - "Upload streams file to disk without loading entire file in memory"
    - "Docling client calls docling-serve API with exponential backoff retry"
    - "Background task updates document status as processing progresses"
    - "docling-serve is running and healthy (pre-condition from Phase 1)"
  artifacts:
    - path: "backend/app/services/docling_client.py"
      provides: "Async docling-serve API client with retry"
      min_lines: 60
    - path: "backend/app/routers/documents.py"
      provides: "Document upload and management endpoints"
      min_lines: 100
  key_links:
    - from: "backend/app/routers/documents.py"
      to: "backend/app/services/docling_client.py"
      via: "background task parse_document call"
      pattern: "BackgroundTasks|add_task"
    - from: "backend/app/services/docling_client.py"
      to: "docling-serve"
      via: "httpx async client"
      pattern: "httpx\\.AsyncClient|/v1/convert"
  preconditions:
    - requirement: "INFRA-08"
      description: "docling-serve deployed as separate service with API endpoint"
      source: "Phase 1 - docker-compose.yml"
      verify: "curl http://localhost:5000/health returns healthy"
---

<objective>
Create document upload endpoint and docling-serve integration with async processing.

Purpose: Enable users to upload documents, validate them, store safely, and trigger async parsing via docling-serve.

Output: Upload endpoint, docling client with retry, background processing pipeline.

**Pre-condition:** docling-serve must be running (deployed in Phase 1 via docker-compose.yml). Verify with `curl http://docling:5000/health` from within the Docker network or `curl http://localhost:5000/health` from host.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing-pipeline/02-RESEARCH.md
@.planning/phases/02-document-processing-pipeline/02-CONTEXT.md
@.planning/phases/02-document-processing-pipeline/02-01-SUMMARY.md
@backend/app/config.py
@backend/app/main.py
@backend/app/middleware/auth.py
@backend/app/models/documents.py
@backend/app/services/storage.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create docling-serve API client with retry logic</name>
  <files>
    backend/app/services/docling_client.py
  </files>
  <action>
Create async docling-serve client with exponential backoff retry:

**backend/app/services/docling_client.py:**

```python
import backoff
import httpx
from pathlib import Path
from typing import Dict, Any, Optional
from app.core.logging import get_logger
from app.config import settings

logger = get_logger()

class DoclingError(Exception):
    """Base exception for docling client errors."""
    pass

class DoclingParseError(DoclingError):
    """Raised when document parsing fails."""
    pass

class DoclingClient:
    """Async client for docling-serve API with retry logic."""

    def __init__(self, base_url: Optional[str] = None, timeout: float = 120.0):
        self.base_url = base_url or settings.DOCLING_URL
        self.timeout = timeout

    def _is_transient_error(self, exception: Exception) -> bool:
        """Determine if error is transient and should be retried."""
        if isinstance(exception, httpx.TimeoutException):
            return True
        if isinstance(exception, httpx.HTTPStatusError):
            return exception.response.status_code >= 500
        return False

    @backoff.on_exception(
        backoff.expo,
        (httpx.TimeoutException, httpx.NetworkError),
        max_tries=3,
        max_time=180,
        on_backoff=lambda details: logger.warning(
            "docling_retry",
            attempt=details["tries"],
            wait=details["wait"]
        )
    )
    async def parse_document(self, file_path: Path) -> Dict[str, Any]:
        """
        Parse document via docling-serve with retry on transient failures.

        Returns docling output with sections, headings, tables structure.
        Raises DoclingParseError on permanent failure.
        """
        logger.info("docling_parse_started", file_path=str(file_path))

        async with httpx.AsyncClient(timeout=self.timeout) as client:
            with open(file_path, 'rb') as f:
                content_type = self._get_content_type(file_path)
                files = {'file': (file_path.name, f, content_type)}

                try:
                    response = await client.post(
                        f"{self.base_url}/v1/convert/file",
                        files=files,
                        params={
                            "to_formats": "json",
                            "do_ocr": "true"
                        }
                    )
                    response.raise_for_status()

                    result = response.json()
                    logger.info("docling_parse_completed",
                               sections=len(result.get("sections", [])))
                    return result

                except httpx.HTTPStatusError as e:
                    if e.response.status_code < 500:
                        # Permanent error (4xx)
                        logger.error("docling_parse_failed",
                                   status_code=e.response.status_code,
                                   response=e.response.text[:500])
                        raise DoclingParseError(f"Docling parse failed: {e.response.status_code}")
                    raise  # Let backoff handle 5xx

    def _get_content_type(self, file_path: Path) -> str:
        """Get MIME type from file extension."""
        ext = file_path.suffix.lower()
        return {
            '.pdf': 'application/pdf',
            '.docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
            '.doc': 'application/msword'
        }.get(ext, 'application/octet-stream')
```

Key implementation details:
- Use backoff decorator for exponential retry on timeouts and 5xx errors
- Give up immediately on 4xx errors (bad request, unsupported format)
- 120 second timeout per request (PDFs can take time)
- Log retry attempts for debugging
- Return structured JSON from docling-serve /v1/convert/file endpoint
  </action>
  <verify>
  - `python -c "from app.services.docling_client import DoclingClient, DoclingError; print('Client OK')"` succeeds
  - Code review: backoff decorator present with max_tries=3
  - Code review: httpx.AsyncClient used (not requests)
  - Verify docling-serve is running: `curl http://localhost:5000/health` returns healthy
  </verify>
  <done>
  - DoclingClient with parse_document method
  - Exponential backoff retry on transient errors (timeout, 5xx)
  - Immediate failure on permanent errors (4xx)
  - Structured logging of parse attempts and results
  </done>
</task>

<task type="auto">
  <name>Task 2: Create document upload endpoint with background processing</name>
  <files>
    backend/app/routers/documents.py
    backend/app/main.py
  </files>
  <action>
Create document upload endpoint that validates, stores, and triggers async processing:

**backend/app/routers/documents.py:**

1. **Imports and setup:**
   - FastAPI Router with prefix `/api/documents`, tags=["documents"]
   - Import auth dependency from app.middleware.auth (get_current_user)
   - Import StorageService, DocumentDatabase, DoclingClient
   - Import BackgroundTasks from fastapi

2. **File validation helper:**
   ```python
   ALLOWED_TYPES = {
       "application/pdf": "pdf",
       "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx"
   }
   MAX_SIZE = settings.MAX_FILE_SIZE_MB * 1024 * 1024

   async def validate_file(file: UploadFile) -> tuple[str, int]:
       """Validate file type and size. Returns (file_type, size) or raises HTTPException."""
       if file.content_type not in ALLOWED_TYPES:
           raise HTTPException(400, f"Invalid file type. Allowed: PDF, DOCX")

       # Read chunks to check size without loading entire file
       size = 0
       chunks = []
       async for chunk in file:
           size += len(chunk)
           if size > MAX_SIZE:
               raise HTTPException(413, f"File exceeds {settings.MAX_FILE_SIZE_MB}MB limit")
           chunks.append(chunk)

       # Reset for later reading
       content = b''.join(chunks)
       return ALLOWED_TYPES[file.content_type], size, content
   ```

3. **POST /api/documents/upload endpoint:**
   - Requires authentication (user_id from JWT)
   - Accepts single file via multipart/form-data
   - Check user document count (HTTPException 400 if >= MAX_DOCUMENTS_PER_USER)
   - Generate doc_id (UUID)
   - Validate file type and size
   - Save to storage via StorageService
   - Create document record in database with status=UPLOADING
   - Add background task for processing
   - Return { doc_id, filename, status: "Uploading" }

4. **Background processing function:**
   ```python
   async def process_document_background(doc_id: str, user_id: str, file_path: Path):
       """Background task to parse, chunk, and index document."""
       db = DocumentDatabase()
       docling = DoclingClient()

       try:
           # Update to PARSING
           await db.update_document(doc_id, user_id,
               status=ProcessingStatus.PARSING,
               current_stage="Parsing")

           logger.info("doc_ingestion_started", doc_id=doc_id, user_id=user_id)

           # Parse via docling
           parse_result = await docling.parse_document(file_path)

           # Store parsed result
           storage = StorageService(settings.DATA_DIR)
           await storage.save_processed_json(user_id, doc_id, parse_result)

           # Update status for next phase (chunking happens in 02-03)
           await db.update_document(doc_id, user_id,
               status=ProcessingStatus.CHUNKING,
               current_stage="Chunking",
               page_count=parse_result.get("page_count"))

       except Exception as e:
           logger.error("doc_ingestion_failed", doc_id=doc_id, error=str(e))
           await db.update_document(doc_id, user_id,
               status=ProcessingStatus.FAILED,
               error=str(e))
   ```

5. **GET /api/documents endpoint:**
   - List all documents for authenticated user
   - Return list of document summaries (doc_id, filename, status, created_at)

6. **DELETE /api/documents/{doc_id} endpoint:**
   - Delete document and all associated files
   - Returns 204 on success, 404 if not found

**backend/app/main.py:**
- Add: `from app.routers import documents`
- Add: `app.include_router(documents.router)`
  </action>
  <verify>
  - Backend starts without errors: `cd backend && python -c "from app.main import app; print('App OK')"`
  - Upload endpoint exists: `curl -X OPTIONS http://localhost:8000/api/documents/upload` shows POST allowed
  - Documents router registered in main.py
  - Verify docling-serve connectivity from backend: `docker-compose exec backend curl http://docling:5000/health`
  </verify>
  <done>
  - POST /api/documents/upload validates and stores files
  - Background task parses via docling-serve
  - GET /api/documents lists user's documents
  - DELETE /api/documents/{doc_id} removes documents
  - 10 document limit per user enforced
  - Status updates flow through processing stages
  </done>
</task>

</tasks>

<verification>
Start the backend and test upload endpoint manually:
```bash
# First, verify docling-serve is healthy (INFRA-08 pre-condition)
curl http://localhost:5000/health
# Expected: healthy response

# Terminal 1: Start backend
cd backend
uvicorn app.main:app --reload

# Terminal 2: Test (need valid JWT token from frontend)
# 1. Get token via frontend login
# 2. Test upload with curl:
curl -X POST http://localhost:8000/api/documents/upload \
  -H "Authorization: Bearer <token>" \
  -F "file=@test.pdf"

# Expected: { "doc_id": "...", "filename": "test.pdf", "status": "Uploading" }

# Check document list
curl http://localhost:8000/api/documents \
  -H "Authorization: Bearer <token>"

# Check logs for doc_ingestion_started event
```
</verification>

<success_criteria>
- Upload endpoint accepts PDF/DOCX files up to 10MB
- File validation rejects invalid types with 400 error
- Oversized files rejected with 413 error
- 10 document limit enforced with 400 error
- Background processing updates document status
- Docling client retries on transient failures
- doc_ingestion_started logged when processing begins
- docling-serve health check passes (pre-condition)
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing-pipeline/02-02-SUMMARY.md`
</output>
