---
phase: 02-document-processing-pipeline
plan: 04
type: execute
wave: 3
depends_on: ["02-02", "02-03"]
files_modified:
  - backend/app/routers/documents.py
  - backend/app/services/pipeline.py
  - backend/app/services/__init__.py
autonomous: false

must_haves:
  truths:
    - "GET /api/documents/{doc_id}/status returns current processing stage and progress"
    - "Processing pipeline chains: upload -> parse -> chunk -> index -> done"
    - "doc_ingestion_completed event logged with duration when processing finishes"
    - "Failed documents have error message accessible via status endpoint"
    - "Status includes estimated time remaining based on page count"
    - "End-to-end flow works: upload PDF -> poll status -> see indexed document"
    - "Status API returns status values: Processing, Indexed, Failed (backend capability for INGEST-10; UI display is Phase 6)"
  artifacts:
    - path: "backend/app/services/pipeline.py"
      provides: "Complete ingestion pipeline orchestration"
      min_lines: 80
    - path: "backend/app/routers/documents.py"
      provides: "Status polling endpoint"
      exports: ["GET /api/documents/{doc_id}/status"]
    - path: "backend/app/services/__init__.py"
      provides: "Service exports including DocumentPipeline"
      contains: "DocumentPipeline"
  key_links:
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/chunker.py"
      via: "chunk_document call"
      pattern: "SemanticChunker|chunk_document"
    - from: "backend/app/services/pipeline.py"
      to: "backend/app/services/indexer.py"
      via: "index_chunks call"
      pattern: "TxtaiIndexer|index_chunks"
    - from: "backend/app/routers/documents.py"
      to: "backend/app/services/pipeline.py"
      via: "background task uses DocumentPipeline"
      pattern: "DocumentPipeline|process_document"
  scope_note: |
    INGEST-10 requires "User can see document status in UI (Processing, Indexed, Failed)".
    Phase 2 delivers the BACKEND capability: GET /api/documents/{doc_id}/status endpoint
    that returns status values. The FRONTEND UI to display this status is Phase 6 scope.
---

<objective>
Create status polling endpoint and complete processing pipeline orchestration.

Purpose: Enable frontend to track document processing progress and complete the ingestion flow from upload through indexing.

Output: Status endpoint, pipeline service, end-to-end verification.

**Scope clarification (INGEST-10):** This plan delivers the backend API that returns document status (Processing, Indexed, Failed). The frontend UI component to display this status to users is delivered in Phase 6 (Frontend Integration). This plan completes the backend contract for INGEST-10.
</objective>

<execution_context>
@C:\Users\Omer\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\Omer\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-document-processing-pipeline/02-RESEARCH.md
@.planning/phases/02-document-processing-pipeline/02-CONTEXT.md
@.planning/phases/02-document-processing-pipeline/02-01-SUMMARY.md
@.planning/phases/02-document-processing-pipeline/02-02-SUMMARY.md
@.planning/phases/02-document-processing-pipeline/02-03-SUMMARY.md
@backend/app/models/documents.py
@backend/app/services/docling_client.py
@backend/app/services/chunker.py
@backend/app/services/indexer.py
@backend/app/routers/documents.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create processing pipeline service and wire to upload endpoint</name>
  <files>
    backend/app/services/pipeline.py
    backend/app/services/__init__.py
    backend/app/routers/documents.py
  </files>
  <action>
Create complete document processing pipeline, export it, and replace the placeholder background function:

**backend/app/services/pipeline.py:**

```python
import time
from pathlib import Path
from typing import Optional
from datetime import datetime, timezone

from app.core.logging import get_logger
from app.core.database import DocumentDatabase
from app.services.docling_client import DoclingClient, DoclingError
from app.services.chunker import SemanticChunker
from app.services.indexer import TxtaiIndexer
from app.services.storage import StorageService
from app.models.documents import ProcessingStatus, ProcessingLogEntry
from app.config import settings

logger = get_logger()


class DocumentPipeline:
    """
    Orchestrates document processing: parse -> chunk -> index.

    Updates document status at each stage and logs processing events.
    """

    def __init__(self):
        self.db = DocumentDatabase()
        self.docling = DoclingClient()
        self.chunker = SemanticChunker()
        self.indexer = TxtaiIndexer()
        self.storage = StorageService(settings.DATA_DIR)

    async def process_document(
        self,
        doc_id: str,
        user_id: str,
        file_path: Path
    ) -> bool:
        """
        Process document through complete pipeline.

        Stages:
        1. PARSING - Call docling-serve to extract structure
        2. CHUNKING - Apply semantic chunking
        3. INDEXING - Store in txtai
        4. DONE - Complete

        Returns True if successful, False if failed.
        """
        pipeline_start = time.time()
        processing_log = []

        logger.info("doc_ingestion_started",
                   doc_id=doc_id,
                   user_id=user_id,
                   file_path=str(file_path))

        try:
            # Stage 1: PARSING
            stage_start = time.time()
            await self._update_status(doc_id, user_id, ProcessingStatus.PARSING, "Parsing document with docling")

            parse_result = await self.docling.parse_document(file_path)

            # Save parsed output
            await self.storage.save_processed_json(user_id, doc_id, parse_result)

            processing_log.append(ProcessingLogEntry(
                stage="Parsing",
                started_at=datetime.fromtimestamp(stage_start, timezone.utc),
                completed_at=datetime.now(timezone.utc),
                duration_ms=int((time.time() - stage_start) * 1000)
            ))

            # Extract page count if available
            page_count = parse_result.get("page_count", len(parse_result.get("pages", [])))

            # Stage 2: CHUNKING
            stage_start = time.time()
            await self._update_status(doc_id, user_id, ProcessingStatus.CHUNKING, "Creating semantic chunks")

            doc = await self.db.get_document(doc_id, user_id)
            chunks = self.chunker.chunk_document(
                parse_result,
                doc_id,
                user_id,
                doc.filename
            )

            processing_log.append(ProcessingLogEntry(
                stage="Chunking",
                started_at=datetime.fromtimestamp(stage_start, timezone.utc),
                completed_at=datetime.now(timezone.utc),
                duration_ms=int((time.time() - stage_start) * 1000)
            ))

            # Stage 3: INDEXING
            stage_start = time.time()
            await self._update_status(doc_id, user_id, ProcessingStatus.INDEXING, "Indexing chunks in txtai")

            indexed_count = self.indexer.index_chunks(chunks, user_id, doc_id)

            processing_log.append(ProcessingLogEntry(
                stage="Indexing",
                started_at=datetime.fromtimestamp(stage_start, timezone.utc),
                completed_at=datetime.now(timezone.utc),
                duration_ms=int((time.time() - stage_start) * 1000)
            ))

            # Stage 4: DONE
            total_duration_ms = int((time.time() - pipeline_start) * 1000)

            await self.db.update_document(
                doc_id, user_id,
                status=ProcessingStatus.DONE,
                current_stage="Complete",
                page_count=page_count,
                chunk_count=indexed_count,
                processing_log=processing_log
            )

            logger.info("doc_ingestion_completed",
                       doc_id=doc_id,
                       user_id=user_id,
                       duration_ms=total_duration_ms,
                       page_count=page_count,
                       chunk_count=indexed_count)

            return True

        except DoclingError as e:
            await self._handle_failure(doc_id, user_id, f"Document parsing failed: {e}", processing_log)
            return False

        except Exception as e:
            logger.exception("pipeline_error", doc_id=doc_id, error=str(e))
            await self._handle_failure(doc_id, user_id, f"Processing error: {e}", processing_log)
            return False

    async def _update_status(
        self,
        doc_id: str,
        user_id: str,
        status: ProcessingStatus,
        current_stage: str
    ):
        """Update document status in database."""
        await self.db.update_document(
            doc_id, user_id,
            status=status,
            current_stage=current_stage
        )

    async def _handle_failure(
        self,
        doc_id: str,
        user_id: str,
        error: str,
        processing_log: list
    ):
        """Handle pipeline failure."""
        logger.error("doc_ingestion_failed", doc_id=doc_id, error=error)

        await self.db.update_document(
            doc_id, user_id,
            status=ProcessingStatus.FAILED,
            current_stage="Failed",
            error=error,
            processing_log=processing_log
        )

        # Clean up files on failure (per CONTEXT.md decision)
        try:
            self.storage.delete_document_files(user_id, doc_id)
        except Exception as e:
            logger.warning("cleanup_failed", doc_id=doc_id, error=str(e))


# Stage weights for progress estimation
STAGE_WEIGHTS = {
    "Uploading": 0.1,
    "Parsing": 0.4,
    "Chunking": 0.2,
    "Indexing": 0.3,
    "Complete": 1.0
}


def calculate_progress(status: ProcessingStatus, current_stage: str) -> int:
    """Calculate progress percentage based on current stage."""
    if status == ProcessingStatus.DONE:
        return 100
    if status == ProcessingStatus.FAILED:
        return 0

    completed = 0.0
    for stage, weight in STAGE_WEIGHTS.items():
        if stage == current_stage:
            completed += weight * 0.5  # Assume halfway through current stage
            break
        completed += weight

    return min(99, int(completed * 100))


def estimate_time_remaining(
    current_stage: str,
    page_count: Optional[int] = None
) -> int:
    """
    Estimate seconds remaining based on stage and page count.

    Baseline: ~2 seconds per page (conservative estimate from RESEARCH.md)
    """
    if current_stage in ("Complete", "Failed"):
        return 0

    # Default to 30 pages if unknown
    pages = page_count or 30
    total_estimated = pages * 2  # 2 sec/page

    # Calculate remaining based on stage weights
    remaining_weight = 0.0
    found_stage = False
    for stage, weight in STAGE_WEIGHTS.items():
        if stage == current_stage:
            remaining_weight += weight * 0.5  # Half of current stage
            found_stage = True
        elif found_stage:
            remaining_weight += weight

    return max(0, int(total_estimated * remaining_weight))
```

**backend/app/services/__init__.py:**

Create or update the services __init__.py to export DocumentPipeline:

```python
"""Services module exports."""

from app.services.pipeline import DocumentPipeline, calculate_progress, estimate_time_remaining
from app.services.docling_client import DoclingClient, DoclingError, DoclingParseError
from app.services.chunker import SemanticChunker
from app.services.indexer import TxtaiIndexer
from app.services.storage import StorageService

__all__ = [
    "DocumentPipeline",
    "calculate_progress",
    "estimate_time_remaining",
    "DoclingClient",
    "DoclingError",
    "DoclingParseError",
    "SemanticChunker",
    "TxtaiIndexer",
    "StorageService",
]
```

**backend/app/routers/documents.py (update to use DocumentPipeline):**

**IMPORTANT:** Replace the existing `process_document_background` function created in Plan 02-02 with the pipeline version:

1. Add import at top:
   ```python
   from app.services.pipeline import DocumentPipeline, calculate_progress, estimate_time_remaining
   ```

2. **REPLACE** the existing `process_document_background` function with:
   ```python
   async def process_document_background(doc_id: str, user_id: str, file_path: Path):
       """
       Background task to process document through complete pipeline.

       This replaces the placeholder parsing-only implementation from Plan 02-02.
       Now uses DocumentPipeline for full flow: parse -> chunk -> index.
       """
       pipeline = DocumentPipeline()
       await pipeline.process_document(doc_id, user_id, file_path)
   ```

   The key change: Instead of calling DoclingClient directly and only handling parsing,
   we now delegate to DocumentPipeline which handles the complete flow including
   chunking (via SemanticChunker from 02-03) and indexing (via TxtaiIndexer from 02-03).

3. Add GET /api/documents/{doc_id}/status endpoint:
   ```python
   @router.get("/{doc_id}/status")
   async def get_document_status(
       doc_id: str,
       user: dict = Depends(get_current_user)
   ):
       """
       Get document processing status with progress and time estimate.

       Returns status values for INGEST-10:
       - "Processing" (maps to Uploading, Parsing, Chunking, Indexing stages)
       - "Indexed" (maps to Done status)
       - "Failed" (maps to Failed status)

       Note: Frontend UI to display this status is implemented in Phase 6.
       """
       user_id = user["sub"]

       db = DocumentDatabase()
       doc = await db.get_document(doc_id, user_id)

       if not doc:
           raise HTTPException(404, "Document not found")

       # Map internal status to INGEST-10 status values
       if doc.status == ProcessingStatus.DONE:
           display_status = "Indexed"
       elif doc.status == ProcessingStatus.FAILED:
           display_status = "Failed"
       else:
           display_status = "Processing"

       return {
           "doc_id": doc.doc_id,
           "filename": doc.filename,
           "status": display_status,  # INGEST-10 compliant: Processing, Indexed, Failed
           "internal_status": doc.status.value,  # Detailed status for debugging
           "current_stage": doc.current_stage,
           "progress_pct": calculate_progress(doc.status, doc.current_stage),
           "estimated_time_remaining": estimate_time_remaining(doc.current_stage, doc.page_count),
           "page_count": doc.page_count,
           "chunk_count": doc.chunk_count,
           "processing_log": [
               {
                   "stage": entry.stage,
                   "started_at": entry.started_at.isoformat(),
                   "completed_at": entry.completed_at.isoformat() if entry.completed_at else None,
                   "duration_ms": entry.duration_ms
               }
               for entry in doc.processing_log
           ],
           "error": doc.error if doc.status == ProcessingStatus.FAILED else None,
           "created_at": doc.created_at.isoformat(),
           "updated_at": doc.updated_at.isoformat()
       }
   ```
  </action>
  <verify>
  - `python -c "from app.services.pipeline import DocumentPipeline, calculate_progress, estimate_time_remaining; print('Pipeline OK')"` succeeds
  - `python -c "from app.services import DocumentPipeline; print('Export OK')"` succeeds (tests __init__.py export)
  - Status endpoint accessible: `curl http://localhost:8000/api/documents/test-doc/status -H "Authorization: Bearer <token>"` returns document status
  - Status response includes `status` field with values: "Processing", "Indexed", or "Failed" (INGEST-10 compliant)
  - Grep documents.py for "DocumentPipeline" to confirm background function uses pipeline
  </verify>
  <done>
  - DocumentPipeline orchestrates parse -> chunk -> index flow
  - DocumentPipeline exported from backend/app/services/__init__.py
  - process_document_background in documents.py REPLACED to use DocumentPipeline (not the placeholder)
  - Status endpoint returns progress_pct and estimated_time_remaining
  - Status endpoint returns INGEST-10 compliant status values (Processing, Indexed, Failed)
  - Processing log tracks stage durations
  - doc_ingestion_completed event logged with metrics
  - Failure handling cleans up files
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete document processing pipeline: upload, parse via docling, semantic chunk, index in txtai, status polling with INGEST-10 compliant status values (Processing, Indexed, Failed)</what-built>
  <how-to-verify>
**Prerequisites:**
1. Docker Compose running: `docker-compose up -d`
2. All services healthy: `docker-compose ps` (should show healthy for backend, docling)
3. **Verify docling-serve is running:** `curl http://localhost:5000/health` (must return healthy)

**Test Flow:**

1. **Login to get auth token:**
   - Open http://localhost:3000
   - Login with test credentials (or register new user)
   - Open browser DevTools > Network
   - Navigate to Dashboard
   - Find a request to /api/auth/backend-token and copy the JWT from response

2. **Upload a PDF document:**
   ```bash
   curl -X POST http://localhost:8000/api/documents/upload \
     -H "Authorization: Bearer <your-jwt-token>" \
     -F "file=@test.pdf"
   ```
   Expected: `{"doc_id": "...", "filename": "test.pdf", "status": "Uploading"}`

3. **Poll status every 2 seconds:**
   ```bash
   # Replace <doc_id> with actual doc_id from upload response
   watch -n 2 'curl -s http://localhost:8000/api/documents/<doc_id>/status \
     -H "Authorization: Bearer <token>" | jq .'
   ```
   Expected: See status field show "Processing" then eventually "Indexed"

4. **Verify INGEST-10 status values:**
   - During processing: `"status": "Processing"`
   - After completion: `"status": "Indexed"`
   - On failure: `"status": "Failed"`
   - Also verify `internal_status` shows detailed stage (Uploading, Parsing, Chunking, Indexing, Done)

5. **Verify final status:**
   - status: "Indexed" (INGEST-10 compliant)
   - chunk_count > 0
   - processing_log shows all stages with durations
   - No error field

6. **Verify logs:**
   ```bash
   docker-compose logs backend | grep -E "doc_ingestion_(started|completed)"
   ```
   Expected: See both events with doc_id, user_id, and duration_ms

7. **List documents:**
   ```bash
   curl http://localhost:8000/api/documents \
     -H "Authorization: Bearer <token>"
   ```
   Expected: Array containing your uploaded document

**Error Testing:**
- Upload non-PDF file: Should get 400 error
- Upload file > 10MB: Should get 413 error
- Upload 11th document: Should get 400 error (limit exceeded)
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe any issues encountered</resume-signal>
</task>

</tasks>

<verification>
**Pre-condition check (WARNING 2 addressed):**
Before running end-to-end tests, verify docling-serve is healthy:
```bash
curl http://localhost:5000/health
# Expected: healthy response
# If fails: docker-compose up -d docling && sleep 30 && retry
```

Complete end-to-end test:
1. Upload PDF document
2. Poll status until "Indexed" (INGEST-10 compliant status)
3. Verify chunk_count > 0
4. Check logs for ingestion events
5. List documents shows indexed document
</verification>

<success_criteria>
- Document uploads successfully with status tracking
- Status endpoint shows accurate progress percentage
- Status endpoint returns INGEST-10 compliant values: Processing, Indexed, Failed
- Processing completes with chunk_count populated
- doc_ingestion_started and doc_ingestion_completed logged
- Error cases handled with user-friendly messages
- 10 document per user limit enforced
- Backend contract for INGEST-10 complete (UI display is Phase 6)
- DocumentPipeline importable from backend/app/services
</success_criteria>

<output>
After completion, create `.planning/phases/02-document-processing-pipeline/02-04-SUMMARY.md`
</output>
