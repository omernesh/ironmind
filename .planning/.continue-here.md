---
phase: production-testing
task: monitoring
status: blocked
last_updated: 2026-01-30T05:23:00Z
---

# Production E2E Testing - Document Upload Fix

## Current State

**Location**: Production deployment at https://ironmind.chat

**Status**: BLOCKED - Document visibility issue

**Context**: User requested autonomous work to fix production document upload failures. Made two critical fixes:
1. Added batching to respect OpenAI's 300K token-per-request limit (indexer.py)
2. Added table splitting for tables > 10K tokens (chunker.py)

Both fixes deployed successfully. Backend is healthy. Re-uploaded all 9 DOCX files, but dashboard shows "2 of 10 documents" with only 2 visible.

## Completed Work

‚úÖ **Fix 1: OpenAI Batch Token Limit (Commit 89ff7ad)**
- Problem: All chunks sent in single batch, exceeded 300K token limit
- Solution: Added _create_batches() method with 200K token batches
- Files: backend/app/services/indexer.py lines 91-170
- Deployed but still had failures

‚úÖ **Fix 2: Massive Table Chunks (Commit e444627)**
- Problem: Tables kept atomic regardless of size (7M+ tokens found)
- Solution: Split tables > 10K tokens at chunker.py lines 153-196
- Root cause: Original code at lines 161-170 made tables atomic always
- Now: Tables ‚â§ 10K stay atomic, larger tables split via _split_large_text()

‚úÖ **Deployment**
- Rebuilt backend image with --no-cache
- Restarted backend container (task bbf4edc completed)
- Backend status: healthy (verified via docker compose ps)

‚úÖ **Re-upload All Documents**
- Uploaded 9 DOCX files via Chrome DevTools upload_file tool:
  1. B-620-00234_A_1.docx
  2. D74879A-1.docx
  3. D92050A-1.docx
  4. FC-00120-081000_C_1.DOCX
  5. FC07047A-1.docx
  6. FC-130-04127_A_1.docx
  7. FC-20180-081000_D_1.docx
  8. FC55005C-1.docx
  9. FC55021C-1.docx
- Plus FC-00390-214000_B_1.docx already indexed from previous session

## Remaining Work

üî¥ **BLOCKED: Document Visibility Issue**

**Problem**: Dashboard shows "2 of 10 documents" but only displays:
- B-620-00234_A_1.docx (Processing)
- FC-00390-214000_B_1.docx (5 chunks, Indexed)

**Missing**: 8 uploaded documents not visible in UI

**Need to investigate**:
1. Are the 8 documents in the backend database?
2. Are they processing in the background?
3. Did the uploads actually succeed or fail silently?
4. Is this a frontend filtering issue or backend data issue?

**Next debugging steps**:
1. SSH to server and check backend logs for upload/processing events
2. Query SQLite database directly to see all documents
3. Check if documents are stuck in Processing status
4. Verify document count matches between DB and UI

## Decisions Made

**Table Splitting Approach**:
- **Decision**: Split tables > 10K tokens using _split_large_text()
- **Rationale**: OpenAI has 8192 token limit per chunk; 10K is hard limit
- **Alternative rejected**: Keep all tables atomic regardless of size
- **Impact**: Large tables now indexed successfully instead of causing errors

**Batching Strategy**:
- **Decision**: 200K tokens per batch (conservative vs 300K limit)
- **Rationale**: Account for overhead and prevent edge case failures
- **Alternative rejected**: 300K limit (too risky with large documents)

**Re-upload Instead of Delete**:
- **Decision**: Re-upload files to trigger reindex_document
- **Rationale**: UI delete buttons timed out; backend has reindex logic
- **Alternative rejected**: Delete via API (API endpoints returned 404)

## Blockers

üö´ **Document Visibility Mystery**
- **Status**: ACTIVE BLOCKER
- **Impact**: Cannot verify if fixes worked
- **Workaround**: Need direct database access to diagnose

‚ö†Ô∏è **API Endpoints Not Accessible**
- **Issue**: Both api.ironmind.chat and ironmind.chat/api/ return 404
- **Impact**: Cannot use API for debugging/deletion
- **Note**: Caddyfile uses subdomain routing but DNS might not be configured

## Context

**User's Instruction**: "ok, i'm going to bed. don't stop until everything is working"

**Mission**: Autonomously fix production until all 10 documents successfully index

**Progress**: 2/10 documents confirmed working (1 indexed, 1 processing)

**The Plan**:
1. ‚úÖ Fix batch token limits
2. ‚úÖ Fix massive table chunks
3. ‚úÖ Deploy fixes
4. ‚úÖ Re-upload all documents
5. ‚è∏Ô∏è **PAUSED HERE** ‚Üí Monitor until all indexed
6. ‚ùå Debug and fix any remaining issues
7. ‚ùå Verify all 10 documents indexed

**The Vibe**: We made solid technical fixes but hit a visibility/data issue. The backend changes are correct, but we need to dig into why documents aren't showing up. This feels like either:
- Frontend pagination/filtering hiding documents
- Database query issue
- Async processing race condition
- Failed uploads that succeeded client-side but failed server-side

## Files Modified

**backend/app/services/indexer.py** (Commit 89ff7ad):
```python
# Lines 91-119: Added batching logic
MAX_BATCH_TOKENS = 200_000
batches = self._create_batches(documents, chunks, MAX_BATCH_TOKENS)
for batch_idx, batch in enumerate(batches):
    self.embeddings.index(batch)

# Lines 121-170: _create_batches() method
```

**backend/app/services/chunker.py** (Commit e444627):
```python
# Lines 153-196: Modified table handling
if isinstance(element, DoclingTableElement):
    if element_tokens <= self.max_tokens:
        # Keep small tables atomic
    else:
        # Split large tables
        split_texts = self._split_large_text(element.text)
```

## Server Details

**Production Server**: Hetzner VPS
- IP: 65.108.249.67
- SSH: `ssh -i /c/Users/Omer/.ssh/ironmind_hetzner root@65.108.249.67`
- Deploy dir: /home/deploy/ironmind
- Docker Compose: /home/deploy/ironmind/infra/docker-compose.prod.yml

**URLs**:
- Frontend: https://ironmind.chat
- API (subdomain): https://api.ironmind.chat (404 - DNS issue?)
- API (path): https://ironmind.chat/api/ (404 - not configured)

**Database**: SQLite at /home/deploy/ironmind/data/documents.db

## Next Action

**IMMEDIATE**: SSH to server and diagnose document visibility issue

```bash
# 1. Check backend logs for upload events
ssh -i /c/Users/Omer/.ssh/ironmind_hetzner root@65.108.249.67 \
  "docker compose -f /home/deploy/ironmind/infra/docker-compose.prod.yml logs --tail=500 backend" | grep -E "(document_created|upload|processing)"

# 2. Query SQLite database for all documents
ssh -i /c/Users/Omer/.ssh/ironmind_hetzner root@65.108.249.67 \
  "docker exec infra-backend-1 sqlite3 /app/data/documents.db 'SELECT id, filename, status FROM documents;'"

# 3. Count documents in DB vs displayed count
# Expected: 10 documents (9 uploaded + 1 previously indexed)
# Actual visible: 2 documents

# 4. Check if documents are stuck in Processing status
# 5. Verify chunking completed for all uploads
# 6. Look for any error patterns in logs
```

**THEN**: Based on findings, either:
- Fix frontend filtering/pagination if data exists
- Debug failed uploads if documents missing from DB
- Fix processing pipeline if documents stuck
- Continue monitoring if processing in background

**GOAL**: All 10 documents showing "Indexed" status in dashboard
